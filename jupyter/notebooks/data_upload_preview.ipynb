{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals before Sunday\n",
    "\n",
    "- Create Metadata Manager\n",
    "    - Enable querying of Notion\n",
    "    - Show what exists in Notion vs Postgres\n",
    "    - Import from Notion\n",
    "    ```python\n",
    "    from services.metadata import MetadataManager\n",
    "    from services.file_uploader import DiveDBUploader\n",
    "\n",
    "    uploader = DiveDBUploader(configs)\n",
    "    metadata_manager = MetadataManager(configs?)\n",
    "    metadata_manager.list_models(model_type=(\"animal\", \"deployment\", \"animal_deployment\", \"logger\", \"recording\"))\n",
    "    metadata_manager.compare_to_notion(model_type=(\"animal\", \"deployment\", \"animal_deployment\", \"logger\", \"recording\"))\n",
    "    metadata_manager.import_from_notion(model_type=(\"animal\", \"deployment\", \"animal_deployment\", \"logger\", \"recording\"))\n",
    "    ```    \n",
    "\n",
    "- Update File Uploader\n",
    "    - Add hypno upload\n",
    "    - Add edf upload\n",
    "        - Use csv_metadata_map to map the metadata to the edf file\n",
    "    - Write to OpenStack \n",
    "    - Write to Postgres\n",
    "\n",
    "    ```python\n",
    "    uploader.upload_edf(edf_file_path, csv_metadata_path, csv_metadata_map)\n",
    "    uploader.upload_hypno(csv_hypno_path, type=(\"graph\"|\"gram\"))\n",
    "    ```\n",
    "\n",
    "    - Include some prompts with questions for the uploader\n",
    "\n",
    "- Update Delta Lake\n",
    "    - Read from Delta Lake using metadata\n",
    "    - Write to OpenStack \n",
    "    - Import from OpenStack by Metadata\n",
    "    ```python\n",
    "    ducklake.read_from_delta(\n",
    "        animal_ids=[],\n",
    "        deployment_ids=[],\n",
    "        logger_ids=[],\n",
    "        recording_ids=[],\n",
    "        signal_names=[]\n",
    "    )\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import mne\n",
    "from netCDF4 import Dataset\n",
    "import json\n",
    "import dask.array as da\n",
    "from dask import delayed\n",
    "import os, logging\n",
    "from services.utils.timing import TimingContext\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "from services.delta_lake import Duck_Lake\n",
    "from services.utils.directory_utils import get_tmpdir\n",
    "from prefect import flow, task\n",
    "from prefect_dask import DaskTaskRunner\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List\n",
    "from uuid import uuid4 as uuid\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "import django\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.root.setLevel(logging.INFO)\n",
    "\n",
    "ducklake = Duck_Lake()\n",
    "\n",
    "os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"server.django_app.settings\")\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"\n",
    "django.setup()\n",
    "\n",
    "from server.metadata.models import (\n",
    "    Deployments,\n",
    "    Loggers,\n",
    "    Animals,\n",
    "    Recordings,\n",
    "    AnimalDeployments,\n",
    "    Files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">18:06:46.796 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | root - Starting Main...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "18:06:46.796 | \u001b[36mINFO\u001b[0m    | root - Starting Main...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">18:06:46.799 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | root - Starting EDF Read...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "18:06:46.799 | \u001b[36mINFO\u001b[0m    | root - Starting EDF Read...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /data/files/test33_HypoactiveHeidi_05_DAY1_PROCESSED.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3405/3116539976.py:87: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  raw = mne.io.read_raw_edf(edf_file_path, preload=False)\n",
      "/usr/local/lib/python3.12/site-packages/mne/io/edf/edf.py:782: RuntimeWarning: All-NaN axis encountered\n",
      "  value = np.nanmax([_prefilter_float(x) for x in values])\n",
      "/tmp/ipykernel_3405/3116539976.py:87: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  raw = mne.io.read_raw_edf(edf_file_path, preload=False)\n",
      "/usr/local/lib/python3.12/site-packages/mne/io/edf/edf.py:784: RuntimeWarning: All-NaN axis encountered\n",
      "  value = np.nanmin([_prefilter_float(x) for x in values])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">18:06:47.218 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | root - ECG_Raw_Ch1\n",
       "</pre>\n"
      ],
      "text/plain": [
       "18:06:47.218 | \u001b[36mINFO\u001b[0m    | root - ECG_Raw_Ch1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /data/files/test33_HypoactiveHeidi_05_DAY1_PROCESSED.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3405/3116539976.py:53: UserWarning: no explicit representation of timezones available for np.datetime64\n",
      "  start_time = np.datetime64(raw.info[\"meas_date\"])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">18:06:50.027 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | root - SignalMetadata(signal_name=['ECG_Raw_Ch1'], frequency=[500.0], start_time=[np.datetime64('2021-04-20T18:02:08.000000')], end_time=[np.datetime64('2021-04-21T18:02:07.998000')])\n",
       "</pre>\n"
      ],
      "text/plain": [
       "18:06:50.027 | \u001b[36mINFO\u001b[0m    | root - SignalMetadata(signal_name=['ECG_Raw_Ch1'], frequency=[500.0], start_time=[np.datetime64('2021-04-20T18:02:08.000000')], end_time=[np.datetime64('2021-04-21T18:02:07.998000')])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/django/db/models/fields/__init__.py:1665: RuntimeWarning: DateTimeField Recordings.end_time received a naive datetime (2021-04-21 18:02:07.998000) while time zone support is active.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/site-packages/django/db/models/fields/__init__.py:1665: RuntimeWarning: DateTimeField Recordings.start_time received a naive datetime (2021-04-20 18:02:08) while time zone support is active.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">18:06:56.044 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | root - Finished EDF Read in 9.24 seconds.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "18:06:56.044 | \u001b[36mINFO\u001b[0m    | root - Finished EDF Read in 9.24 seconds.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">18:06:56.047 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | root - Finished Main in 9.25 seconds.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "18:06:56.047 | \u001b[36mINFO\u001b[0m    | root - Finished Main in 9.25 seconds.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot assign \"(<Recordings: Recordings object (20210420_oror-002_CC-96)>, True)\": \"Files.recording\" must be a \"Recordings\" instance.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 168\u001b[0m\n\u001b[1;32m    157\u001b[0m MetadataSchema \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39mschema(\n\u001b[1;32m    158\u001b[0m     [\n\u001b[1;32m    159\u001b[0m         pa\u001b[38;5;241m.\u001b[39mfield(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msignal_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, pa\u001b[38;5;241m.\u001b[39mstring()),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m     ]\n\u001b[1;32m    164\u001b[0m )\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TimingContext(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMain\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 168\u001b[0m     \u001b[43mprocess_edf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_edf_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDataSchema\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 121\u001b[0m, in \u001b[0;36mprocess_edf\u001b[0;34m(edf_file_path, schema)\u001b[0m\n\u001b[1;32m    113\u001b[0m recording \u001b[38;5;241m=\u001b[39m Recordings\u001b[38;5;241m.\u001b[39mobjects\u001b[38;5;241m.\u001b[39mget_or_create(\n\u001b[1;32m    114\u001b[0m     animal_deployment\u001b[38;5;241m=\u001b[39manimal_deployment,\n\u001b[1;32m    115\u001b[0m     logger\u001b[38;5;241m=\u001b[39mlogger,\n\u001b[1;32m    116\u001b[0m     start_time\u001b[38;5;241m=\u001b[39mnumpy_datetime64_to_datetime(signalMetadata\u001b[38;5;241m.\u001b[39mstart_time[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS.\u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    117\u001b[0m     end_time\u001b[38;5;241m=\u001b[39mnumpy_datetime64_to_datetime(signalMetadata\u001b[38;5;241m.\u001b[39mend_time[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS.\u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    118\u001b[0m )\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Create a new file\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[43mFiles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobjects\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrecording\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecording\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medf_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43medf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    126\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m ducklake\u001b[38;5;241m.\u001b[39mwrite_to_delta(\n\u001b[1;32m    129\u001b[0m     data\u001b[38;5;241m=\u001b[39mtable,\n\u001b[1;32m    130\u001b[0m     schema\u001b[38;5;241m=\u001b[39mschema,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    135\u001b[0m )\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m table, signalData\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/django/db/models/manager.py:87\u001b[0m, in \u001b[0;36mBaseManager._get_queryset_methods.<locals>.create_method.<locals>.manager_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(method)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmanager_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_queryset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/django/db/models/query.py:677\u001b[0m, in \u001b[0;36mQuerySet.create\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reverse_one_to_one_fields:\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    673\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following fields do not exist in this model: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    674\u001b[0m         \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(reverse_one_to_one_fields)\n\u001b[1;32m    675\u001b[0m     )\n\u001b[0;32m--> 677\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_for_write \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    679\u001b[0m obj\u001b[38;5;241m.\u001b[39msave(force_insert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, using\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/django/db/models/base.py:543\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_related_object:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;66;03m# If we are passed a related instance, set it using the\u001b[39;00m\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;66;03m# field.name instead of field.attname (e.g. \"user\" instead of\u001b[39;00m\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;66;03m# \"user_id\") so that the object gets properly cached (and type\u001b[39;00m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;66;03m# checked) by the RelatedObjectDescriptor.\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rel_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _DEFERRED:\n\u001b[0;32m--> 543\u001b[0m         \u001b[43m_setattr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _DEFERRED:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/django/db/models/fields/related_descriptors.py:287\u001b[0m, in \u001b[0;36mForwardManyToOneDescriptor.__set__\u001b[0;34m(self, instance, value)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# An object must be an instance of the related class.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    285\u001b[0m     value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfield\u001b[38;5;241m.\u001b[39mremote_field\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_meta\u001b[38;5;241m.\u001b[39mconcrete_model\n\u001b[1;32m    286\u001b[0m ):\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot assign \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m must be a \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m instance.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    290\u001b[0m             value,\n\u001b[1;32m    291\u001b[0m             instance\u001b[38;5;241m.\u001b[39m_meta\u001b[38;5;241m.\u001b[39mobject_name,\n\u001b[1;32m    292\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfield\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    293\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfield\u001b[38;5;241m.\u001b[39mremote_field\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_meta\u001b[38;5;241m.\u001b[39mobject_name,\n\u001b[1;32m    294\u001b[0m         )\n\u001b[1;32m    295\u001b[0m     )\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mdb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot assign \"(<Recordings: Recordings object (20210420_oror-002_CC-96)>, True)\": \"Files.recording\" must be a \"Recordings\" instance."
     ]
    }
   ],
   "source": [
    "logging.basicConfig()\n",
    "logging.root.setLevel(logging.INFO)\n",
    "\n",
    "my_edf_file_path = os.path.join(\n",
    "    os.environ[\"CONTAINER_FILE_STORAGE_PATH\"],\n",
    "    \"test33_HypoactiveHeidi_05_DAY1_PROCESSED.edf\",\n",
    ")\n",
    "my_parquet_output_dir = os.path.join(os.environ[\"CONTAINER_FILE_STORAGE_PATH\"], \"test\")\n",
    "\n",
    "# Non-EEG\n",
    "misc_channels = [\n",
    "    \"pitch\",\n",
    "    \"roll\",\n",
    "    \"heading\",\n",
    "    \"GyrZ\",\n",
    "    \"MagZ\",\n",
    "    \"Tag_On\",\n",
    "    \"Depth\",\n",
    "    \"MagX\",\n",
    "    \"MagY\",\n",
    "]\n",
    "\n",
    "@dataclass\n",
    "class SignalMetadata:\n",
    "    signal_name: str\n",
    "    frequency: float\n",
    "    start_time: str\n",
    "    end_time: str\n",
    "\n",
    "@dataclass\n",
    "class SignalData:\n",
    "    signal_name: str\n",
    "    # year: int\n",
    "    # month: int\n",
    "    # day: int\n",
    "    # hour: int\n",
    "    time: str\n",
    "    data: float\n",
    "\n",
    "def numpy_datetime64_to_datetime(np_datetime):\n",
    "    return np_datetime.astype('datetime64[us]').astype(datetime)\n",
    "\n",
    "# @task\n",
    "def read_signal(\n",
    "    edf_file_path,\n",
    "    signal_name,\n",
    "    mode: str = \"SINGLE\"\n",
    "):\n",
    "    \"\"\"Function to read a single signal from an EDF file.\"\"\"\n",
    "    raw = mne.io.read_raw_edf(edf_file_path, include=[signal_name], preload=False)\n",
    "    signal = raw.pick(signal_name).get_data()\n",
    "    data = signal[0]\n",
    "    start_time = np.datetime64(raw.info[\"meas_date\"])\n",
    "    freq = raw.info[\"sfreq\"]\n",
    "    data_indices = np.arange(len(data)) / float(freq)\n",
    "    timedelta_array = (data_indices * 1000000).astype('timedelta64[us]')\n",
    "    times = pa.array((start_time + timedelta_array).astype(float))\n",
    "    end_time = np.datetime64(int(times[-1].as_py()), 'us')\n",
    "    signal_name_arr = np.full(len(times), signal_name)\n",
    "    \n",
    "    if mode == 'SINGLE':\n",
    "        return (\n",
    "            SignalData(\n",
    "                signal_name=signal_name_arr,\n",
    "                # year=year,\n",
    "                # month=month,\n",
    "                # day=day,\n",
    "                # hour=hour,\n",
    "                time=times,\n",
    "                data=data\n",
    "            ),\n",
    "            SignalMetadata(\n",
    "                signal_name=[signal_name],\n",
    "                frequency=[freq],\n",
    "                start_time=[start_time],\n",
    "                end_time=[end_time]\n",
    "            ),\n",
    "        )\n",
    "     \n",
    "\n",
    "# @flow\n",
    "def process_edf(\n",
    "    edf_file_path: str,\n",
    "    schema: pa.schema\n",
    "):\n",
    "    with TimingContext(\"EDF Read\"):\n",
    "        raw = mne.io.read_raw_edf(edf_file_path, preload=False)\n",
    "        \n",
    "        mode=\"overwrite\"\n",
    "        for signal_name in raw.ch_names[0:2]:\n",
    "            logging.info(signal_name)\n",
    "            signalData, signalMetadata = read_signal(edf_file_path, signal_name)\n",
    "            logging.info(signalMetadata)\n",
    "            table = pa.table(asdict(signalData), schema=schema)\n",
    "            \n",
    "            # Find a random animal and deployment and logger\n",
    "            animal = Animals.objects.order_by('?').first()\n",
    "            deployment = Deployments.objects.order_by('?').first()\n",
    "            logger = Loggers.objects.order_by('?').first()\n",
    "            \n",
    "            # Find a random animal and deployment and logger\n",
    "            animal = Animals.objects.order_by('?').first()\n",
    "            deployment = Deployments.objects.order_by('?').first()\n",
    "            logger = Loggers.objects.order_by('?').first()\n",
    "\n",
    "            # Create a new animal deployment (if one doesn't exist)\n",
    "            animal_deployment = AnimalDeployments.objects.create(\n",
    "                animal=animal,\n",
    "                deployment=deployment\n",
    "            )\n",
    "\n",
    "            # Create a new recording\n",
    "            recording = Recordings.objects.get_or_create(\n",
    "                animal_deployment=animal_deployment,\n",
    "                logger=logger,\n",
    "                start_time=numpy_datetime64_to_datetime(signalMetadata.start_time[0]).strftime('%Y-%m-%d %H:%M:%S.%f'),\n",
    "                end_time=numpy_datetime64_to_datetime(signalMetadata.end_time[0]).strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "            )\n",
    "            \n",
    "            # Create a new file\n",
    "            file = Files.objects.create(\n",
    "                recording=recording,\n",
    "                file_path=edf_file_path,\n",
    "                extension=\"edf\",\n",
    "                type=\"data\"\n",
    "            )\n",
    "            \n",
    "            ducklake.write_to_delta(\n",
    "                data=table,\n",
    "                schema=schema,\n",
    "                mode=mode,\n",
    "                partition_by=['signal_name'],\n",
    "                name=file.file_path,\n",
    "                description=\"test\"\n",
    "            )\n",
    "            del table, signalData\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DataSchema = pa.schema(\n",
    "    [\n",
    "        pa.field(\"signal_name\", pa.string()),\n",
    "        # pa.field(\"year\", pa.int16()),\n",
    "        # pa.field(\"month\", pa.int16()),\n",
    "        # pa.field(\"day\", pa.int16()),\n",
    "        # pa.field(\"hour\", pa.int16()),\n",
    "        \n",
    "        # Uncomment if working with dates\n",
    "        # pa.field(\"time\", pa.timestamp('us', tz=\"UTC\")),\n",
    "        pa.field(\"time\", pa.float64()),\n",
    "        pa.field(\"data\", pa.float64()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "MetadataSchema = pa.schema(\n",
    "    [\n",
    "        pa.field(\"signal_name\", pa.string()),\n",
    "        pa.field(\"freq\", pa.int16()),\n",
    "        pa.field(\"start_time\", pa.timestamp('us', tz=\"UTC\")),\n",
    "        pa.field(\"end_time\", pa.timestamp('us', tz=\"UTC\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "uploader = DiveDBUploader(\n",
    "    auth_token=\"\",\n",
    "    metadata_host=\"\",\n",
    "    delta_lake_path=os.environ[\"CONTAINER_DELTA_LAKE_PATH\"]\n",
    ")\n",
    "\n",
    "uploader.lookup_loggers()\n",
    "uploader.set_logger()\n",
    "uploader.create_logger()\n",
    "\n",
    "uploader.lookup_animals()\n",
    "uploader.set_animal()\n",
    "uploader.create_animal()\n",
    "\n",
    "uploader.lookup_adeployemnts()\n",
    "uploader.set_deployemnts()\n",
    "uploader.create_deployemnts()\n",
    "\n",
    "uploader.lookup_recordings()\n",
    "uploader.set_recordings()\n",
    "uploader.create_recordings()\n",
    "\n",
    "\n",
    "uploader.set_metadata(\n",
    "    loggers=None,\n",
    "    animals=None,\n",
    "    deployemnts=None,\n",
    "    recordings=None,\n",
    ")\n",
    "uploader.upload_files()\n",
    "\n",
    "# Add project id to animal model\n",
    "# Load up hypoactive Heidi\n",
    "# Load up another seals\n",
    "# Load up the matching hypotrack (if not hypnogram, then create one)\n",
    "# Heart rate peak detection\n",
    "    # in pyologger Feature generation utils\n",
    "\n",
    "with TimingContext(\"Main\"):\n",
    "    process_edf(my_edf_file_path, DataSchema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db20ef389e3e4895940514559805be90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (43_200_000, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>signal_name</th><th>data</th><th>t</th></tr><tr><td>str</td><td>f64</td><td>i64</td></tr></thead><tbody><tr><td>&quot;ECG_ICA8&quot;</td><td>-0.000031</td><td>1618941728000000</td></tr><tr><td>&quot;ECG_ICA8&quot;</td><td>-0.000033</td><td>1618941728002000</td></tr><tr><td>&quot;ECG_ICA8&quot;</td><td>-0.000032</td><td>1618941728004000</td></tr><tr><td>&quot;ECG_ICA8&quot;</td><td>-0.000032</td><td>1618941728006000</td></tr><tr><td>&quot;ECG_ICA8&quot;</td><td>-0.000031</td><td>1618941728008000</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;ECG_ICA8&quot;</td><td>0.000065</td><td>1619028127990000</td></tr><tr><td>&quot;ECG_ICA8&quot;</td><td>0.000067</td><td>1619028127992000</td></tr><tr><td>&quot;ECG_ICA8&quot;</td><td>0.000067</td><td>1619028127994000</td></tr><tr><td>&quot;ECG_ICA8&quot;</td><td>0.000064</td><td>1619028127996000</td></tr><tr><td>&quot;ECG_ICA8&quot;</td><td>0.000175</td><td>1619028127998000</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (43_200_000, 3)\n",
       "┌─────────────┬───────────┬──────────────────┐\n",
       "│ signal_name ┆ data      ┆ t                │\n",
       "│ ---         ┆ ---       ┆ ---              │\n",
       "│ str         ┆ f64       ┆ i64              │\n",
       "╞═════════════╪═══════════╪══════════════════╡\n",
       "│ ECG_ICA8    ┆ -0.000031 ┆ 1618941728000000 │\n",
       "│ ECG_ICA8    ┆ -0.000033 ┆ 1618941728002000 │\n",
       "│ ECG_ICA8    ┆ -0.000032 ┆ 1618941728004000 │\n",
       "│ ECG_ICA8    ┆ -0.000032 ┆ 1618941728006000 │\n",
       "│ ECG_ICA8    ┆ -0.000031 ┆ 1618941728008000 │\n",
       "│ …           ┆ …         ┆ …                │\n",
       "│ ECG_ICA8    ┆ 0.000065  ┆ 1619028127990000 │\n",
       "│ ECG_ICA8    ┆ 0.000067  ┆ 1619028127992000 │\n",
       "│ ECG_ICA8    ┆ 0.000067  ┆ 1619028127994000 │\n",
       "│ ECG_ICA8    ┆ 0.000064  ┆ 1619028127996000 │\n",
       "│ ECG_ICA8    ┆ 0.000175  ┆ 1619028127998000 │\n",
       "└─────────────┴───────────┴──────────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the recording ID you want to query\n",
    "\n",
    "# Query to get the file path from Django ORM\n",
    "file_record = Files.objects.order_by('?').first()\n",
    "if file_record:\n",
    "    file_path = file_record.file_path\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"No file found for file: {file_record}\")\n",
    "\n",
    "# Now use the retrieved file path in your Delta Lake query\n",
    "df2 = ducklake.conn.execute(\n",
    "    f'''\n",
    "    \n",
    "        SELECT signal_name, data, time::INT64 as t\n",
    "        FROM delta_scan('{os.environ[\"CONTAINER_DELTA_LAKE_PATH\"]}')\n",
    "    \n",
    "    '''\n",
    ").pl()\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df_pivot = df2.pivot(on=\"signal_name\", values=\"data\")\n",
    "df_pivot.plot.line(x=\"t\", y=[\"ECG_ICA8\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
