{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting Main...\n",
      "INFO:root:Starting EDF Read...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete\n",
      "Extracting EDF parameters from /data/files/test33_HypoactiveHeidi_05_DAY1_PROCESSED.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /data/files/test33_HypoactiveHeidi_05_DAY1_PROCESSED.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_911/1647983137.py:74: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  raw = mne.io.read_raw_edf(edf_file_path, preload=False)\n",
      "/tmp/ipykernel_911/1647983137.py:74: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  raw = mne.io.read_raw_edf(edf_file_path, preload=False)\n",
      "/tmp/ipykernel_911/1647983137.py:83: RuntimeWarning: The unit for channel(s) Depth, GyrZ, MagX, MagY, MagZ, Tag_On, heading, pitch, roll has changed from V to NA.\n",
      "  raw.set_channel_types(channel_types)\n",
      "/tmp/ipykernel_911/1647983137.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  raw = mne.io.read_raw_edf(edf_file_path, preload=False)\n",
      "/tmp/ipykernel_911/1647983137.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  raw = mne.io.read_raw_edf(edf_file_path, preload=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /data/files/test33_HypoactiveHeidi_05_DAY1_PROCESSED.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_911/1647983137.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  raw = mne.io.read_raw_edf(edf_file_path, preload=False)\n",
      "/tmp/ipykernel_911/1647983137.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  raw = mne.io.read_raw_edf(edf_file_path, preload=False)\n",
      "INFO:root:[{'signal_name': 'ECG_Raw_Ch1', 'frequency': 500.0, 'start_time': <pyarrow.TimestampScalar: '2021-04-20T18:02:08+0000'>, 'data': array([ 3.96179072e-04,  3.94455253e-04,  3.91199152e-04, ...,\n",
      "        1.01597684e-04,  8.68494607e-05, -4.14792323e-05])}, {'signal_name': 'ECG_ICA8', 'frequency': 500.0, 'start_time': <pyarrow.TimestampScalar: '2021-04-20T18:02:08+0000'>, 'data': array([-3.09471060e-05, -3.25372927e-05, -3.21631312e-05, ...,\n",
      "        6.70832314e-05,  6.43705599e-05,  1.74841770e-04])}]\n",
      "INFO:root:pyarrow.Table\n",
      "signal_name: string\n",
      "frequency: double\n",
      "start_time: timestamp[s, tz=UTC]\n",
      "data: list<item: double>\n",
      "  child 0, item: double\n",
      "----\n",
      "signal_name: [[\"ECG_Raw_Ch1\",\"ECG_ICA8\"]]\n",
      "frequency: [[500,500]]\n",
      "start_time: [[2021-04-20 18:02:08Z,2021-04-20 18:02:08Z]]\n",
      "data: [[[0.0003961790716411078,0.00039445525337605857,0.0003911991522087434,0.0003860276974135958,0.0003860276974135958,...,0.0001397132208743419,0.00012132582604715034,0.00010159768368047605,0.00008684946074616612,-0.00004147923231860841],[-0.00003094710597390707,-0.000032537292744335104,-0.00003216313115129321,-0.00003244375234607463,-0.00003104064637216754,...,0.00006511888303959714,0.00006745739299610894,0.00006708323140306706,0.00006437055985351338,0.0001748417701991302]]]\n",
      "INFO:root:Finished EDF Read in 14.87 seconds.\n",
      "INFO:root:Finished Main in 14.87 seconds.\n"
     ]
    },
    {
     "ename": "SchemaMismatchError",
     "evalue": "Invalid data type for Delta Lake: Timestamp(Second, Some(\"UTC\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSchemaMismatchError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 115\u001b[0m\n\u001b[1;32m    105\u001b[0m schema \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39mschema(\n\u001b[1;32m    106\u001b[0m     [\n\u001b[1;32m    107\u001b[0m         pa\u001b[38;5;241m.\u001b[39mfield(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msignal_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, pa\u001b[38;5;241m.\u001b[39mstring()),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m     ]\n\u001b[1;32m    112\u001b[0m )\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TimingContext(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMain\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 115\u001b[0m     \u001b[43mprocess_edf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_edf_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# # Process each signal separately and write to a Parquet file\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# for signal in signals:\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m#     # Load data for the current signal\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m#     # Write the DataFrame to the Delta Lake table (append mode)\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m#     write_deltalake(\"/path/to/delta-lake\", df, mode=\"append\", partition_by=[\"signal_name\"])\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 95\u001b[0m, in \u001b[0;36mprocess_edf\u001b[0;34m(edf_file_path, schema, misc_channels)\u001b[0m\n\u001b[1;32m     93\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(table)\n\u001b[1;32m     94\u001b[0m ducklake \u001b[38;5;241m=\u001b[39m Duck_Lake()\n\u001b[0;32m---> 95\u001b[0m \u001b[43mducklake\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_to_delta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msignal_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    102\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/app/services/delta_lake.py:32\u001b[0m, in \u001b[0;36mDuck_Lake.write_to_delta\u001b[0;34m(self, data, schema, mode, partition_by, name, description)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite_to_delta\u001b[39m(\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     24\u001b[0m     data: pa\u001b[38;5;241m.\u001b[39mtable,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     description: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     30\u001b[0m ):\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Write data to our delta lake\"\"\"\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta_lake \u001b[38;5;241m=\u001b[39m \u001b[43mwrite_deltalake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtable_or_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelta_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartition_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_by\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/deltalake/writer.py:303\u001b[0m, in \u001b[0;36mwrite_deltalake\u001b[0;34m(table_or_uri, data, schema, partition_by, mode, file_options, max_partitions, max_open_files, max_rows_per_file, min_rows_per_group, max_rows_per_group, name, description, configuration, schema_mode, storage_options, partition_filters, predicate, large_dtypes, engine, writer_properties, custom_metadata, post_commithook_properties)\u001b[0m\n\u001b[1;32m    297\u001b[0m data, schema \u001b[38;5;241m=\u001b[39m _convert_data_and_schema(\n\u001b[1;32m    298\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m    299\u001b[0m     schema\u001b[38;5;241m=\u001b[39mschema,\n\u001b[1;32m    300\u001b[0m     conversion_mode\u001b[38;5;241m=\u001b[39mArrowSchemaConversionMode\u001b[38;5;241m.\u001b[39mPASSTHROUGH,\n\u001b[1;32m    301\u001b[0m )\n\u001b[1;32m    302\u001b[0m data \u001b[38;5;241m=\u001b[39m RecordBatchReader\u001b[38;5;241m.\u001b[39mfrom_batches(schema, (batch \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m data))\n\u001b[0;32m--> 303\u001b[0m \u001b[43mwrite_deltalake_rust\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtable_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtable_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_by\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_table\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfiguration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter_properties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpost_commithook_properties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_commithook_properties\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpost_commithook_properties\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m table:\n\u001b[1;32m    322\u001b[0m     table\u001b[38;5;241m.\u001b[39mupdate_incremental()\n",
      "\u001b[0;31mSchemaMismatchError\u001b[0m: Invalid data type for Delta Lake: Timestamp(Second, Some(\"UTC\"))"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "from netCDF4 import Dataset\n",
    "import json\n",
    "import dask.array as da\n",
    "from dask import delayed\n",
    "import os, logging\n",
    "from services.utils.timing import TimingContext\n",
    "import pyarrow as pa\n",
    "from services.delta_lake import Duck_Lake\n",
    "\n",
    "print(\"Imports complete\")\n",
    "logging.basicConfig()\n",
    "logging.root.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "my_edf_file_path = os.path.join(\n",
    "    os.environ[\"CONTAINER_FILE_STORAGE_PATH\"],\n",
    "    \"test33_HypoactiveHeidi_05_DAY1_PROCESSED.edf\",\n",
    ")\n",
    "my_parquet_output_dir = os.path.join(os.environ[\"CONTAINER_FILE_STORAGE_PATH\"], \"test\")\n",
    "\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SignalSchema:\n",
    "    signal_name: str\n",
    "    frequency: float\n",
    "    start_time: float\n",
    "    data: List[float]\n",
    "\n",
    "\n",
    "def read_signal(\n",
    "    edf_file_path,\n",
    "    signal_name,\n",
    "):\n",
    "    \"\"\"Function to read a single signal from an EDF file.\"\"\"\n",
    "    raw = mne.io.read_raw_edf(edf_file_path, preload=False)\n",
    "    signal = raw.pick(signal_name).get_data()\n",
    "    return SignalSchema(\n",
    "        signal_name=signal_name,\n",
    "        frequency=raw.info[\"sfreq\"],\n",
    "        start_time=pa.scalar(raw.info[\"meas_date\"], type=pa.timestamp('s')),\n",
    "        # start_time=raw.info[\"meas_date\"],\n",
    "        data=signal[0],\n",
    "    )\n",
    "\n",
    "\n",
    "@delayed\n",
    "def delayed_read_signal(edf_file_path, signal_name):\n",
    "    return read_signal(edf_file_path, signal_name)\n",
    "\n",
    "\n",
    "misc_channels = [\n",
    "    \"pitch\",\n",
    "    \"roll\",\n",
    "    \"heading\",\n",
    "    \"GyrZ\",\n",
    "    \"MagZ\",\n",
    "    \"Tag_On\",\n",
    "    \"Depth\",\n",
    "    \"MagX\",\n",
    "    \"MagY\",\n",
    "]\n",
    "\n",
    "\n",
    "def process_edf(\n",
    "    edf_file_path: str,\n",
    "    schema: pa.schema,\n",
    "    misc_channels: List[str] = misc_channels,\n",
    "):\n",
    "    with TimingContext(\"EDF Read\"):\n",
    "        raw = mne.io.read_raw_edf(edf_file_path, preload=False)\n",
    "\n",
    "        channel_types = dict()\n",
    "        \n",
    "        for k in raw.ch_names:\n",
    "            if k in misc_channels:\n",
    "                channel_types[k] = \"misc\"\n",
    "            else:\n",
    "                channel_types[k] = \"eeg\"\n",
    "        raw.set_channel_types(channel_types)\n",
    "        \n",
    "        channels_to_use = [ch for ch in raw.ch_names if ch not in misc_channels]\n",
    "        \n",
    "        buff = []\n",
    "        for signal_name in channels_to_use[0:2]:\n",
    "            signal = read_signal(edf_file_path, signal_name)\n",
    "            buff.append(asdict(signal))\n",
    "        logging.info(buff)\n",
    "        table = pa.Table.from_pylist(buff, schema=schema)\n",
    "        logging.info(table)\n",
    "        ducklake = Duck_Lake()\n",
    "        ducklake.write_to_delta(\n",
    "            data=table,\n",
    "            schema=schema,\n",
    "            mode=\"append\",\n",
    "            partition_by=['signal_name'],\n",
    "            name=\"test\",\n",
    "            description=\"test\"\n",
    "        )\n",
    "\n",
    "\n",
    "schema = pa.schema(\n",
    "    [\n",
    "        pa.field(\"signal_name\", pa.string()),\n",
    "        pa.field(\"frequency\", pa.float64()),\n",
    "        pa.field(\"start_time\", pa.timestamp(\"s\")),\n",
    "        pa.field(\"data\", pa.list_(pa.float64())),\n",
    "    ]\n",
    ")\n",
    "\n",
    "with TimingContext(\"Main\"):\n",
    "    process_edf(my_edf_file_path, schema)\n",
    "\n",
    "\n",
    "# # Process each signal separately and write to a Parquet file\n",
    "# for signal in signals:\n",
    "#     # Load data for the current signal\n",
    "#     signal_data = load_signal_data(signal)\n",
    "\n",
    "#     # Create a PyArrow table from the data\n",
    "#     table = pa.table(signal_data, schema=schema)\n",
    "\n",
    "#     # Write the table to a Parquet file, partitioned by signal name\n",
    "#     pq.write_table(table, f\"{output_parquet_dir}/{signal}.parquet\")\n",
    "\n",
    "# # Once all signals are processed, combine them into a Delta Lake table\n",
    "\n",
    "# # List of Parquet files to be combined\n",
    "# parquet_files = [f\"{output_parquet_dir}/{signal}.parquet\" for signal in signals]\n",
    "\n",
    "# # Convert each Parquet file into a Pandas DataFrame and write to Delta Lake\n",
    "# for parquet_file in parquet_files:\n",
    "#     # Load the Parquet file into a Pandas DataFrame\n",
    "#     df = pd.read_parquet(parquet_file)\n",
    "\n",
    "#     # Write the DataFrame to the Delta Lake table (append mode)\n",
    "#     write_deltalake(\"/path/to/delta-lake\", df, mode=\"append\", partition_by=[\"signal_name\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
