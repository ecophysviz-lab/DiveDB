{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Querying and Exporting\n",
    "\n",
    "This notebook demonstrates the process of querying data from Iceberg data lake (formerly Delta Lake) and exporting it in various formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the servers:\n",
    "To launch the server, open the Docker Desktop app and run the following command at the root of the project:\n",
    "```bash\n",
    "$ make up\n",
    "```\n",
    "This command will launch the Jupyter server using the environment variables defined in the `.env` file.\n",
    "\n",
    "#### Understanding expected file paths:\n",
    "DiveDB expects the following paths to be set in the `.env` file:\n",
    "- `CONTAINER_DATA_PATH`\n",
    "- `LOCAL_DATA_PATH`\n",
    "- `LOCAL_ICEBERG_PATH`\n",
    "- `CONTAINER_ICEBERG_PATH`\n",
    "\n",
    "These paths are used to mount the Iceberg warehouse and file storage to the containers. The \"LOCAL_\" paths can be wherever makes sense for your local machine. The \"CONTAINER_\" paths are the paths that the containers expect. We recommend you keep the \"CONTAINER_\" paths as they are in the `.env.example` file.\n",
    "\n",
    "**Migration Note**: We have migrated from Delta Lake to Apache Iceberg for better performance and schema evolution capabilities.\n",
    "\n",
    "#### When is the server ready?\n",
    "You'll know it's ready when you see the following logs in the terminal:\n",
    "```bash\n",
    "jupyter-1            | [I 2024-08-30 16:12:37.083 ServerApp] Serving notebooks from local directory: /app\n",
    "jupyter-1            | [I 2024-08-30 16:12:37.083 ServerApp] Jupyter Server 2.14.2 is running at:\n",
    "jupyter-1            | [I 2024-08-30 16:12:37.083 ServerApp] http://e29d05e13fd0:8888/jupyter/tree\n",
    "jupyter-1            | [I 2024-08-30 16:12:37.083 ServerApp]     http://127.0.0.1:8888/jupyter/tree\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to the Jupyter Kernel:\n",
    "To connect to the Jupyter server in your notebook, follow these steps:\n",
    "1. Click the \"Select Kernel\" button at the top right of the page.\n",
    "1. Pick the \"Select another kernel\" option in the dropdown menu.\n",
    "1. Pick the \"Existing Jupyter Server\" option in the dropdown menu.\n",
    "1. Now we need to connect to the Jupyter server.\n",
    "    - If you previously connected to the Jupyter server\n",
    "        - Pick the \"localhost\" option in the dropdown menu (or whatever you named it prior)\n",
    "    - If you have not connected to the Jupyter server before\n",
    "        - Pick the \"Enter the URL of the running Jupyter server\" option in the dropdown menu.\n",
    "        - Enter http://localhost:8888/jupyter\n",
    "        - Give it a name you'll remember (like \"Local DiveDB Jupyter Server\")\n",
    "1. Press the \"Reload\" icon in the top right of the dropdown menu to see the latest kernel.\n",
    "1. Pick the \"Python 3\" option in the dropdown menu.\n",
    "\n",
    "This will ensure you execute the Jupyter notebook in the correct environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying from Iceberg Data Lake\n",
    "We connect to our datastores using the `DuckPond` class. DuckPond is a wrapper around a DuckDB connection with access to both our Metadata Database and our measurements stored in Apache Iceberg tables. The ability to query both sources of data from a single connection is useful for quickly accessing data for analysis.\n",
    "\n",
    "Data in our Iceberg data lake is sorted into various views. There's a simple hierarchy to views: datasets are at the highest level and each dataset has a data iceberg, a point events iceberg, and a state events iceberg.\n",
    "```mermaid\n",
    "graph TD\n",
    "    %% Datasets\n",
    "    A[üõ†Ô∏è SS Movement]\n",
    "    B[üõ†Ô∏è NESE Sleep]\n",
    "    C[üõ†Ô∏è OO Physiology]\n",
    "\n",
    "    %% Databases\n",
    "    D1[üßä data]\n",
    "    D2[üßä point events]\n",
    "    D3[üßä state events]\n",
    "\n",
    "    E1[üßä data]\n",
    "    E2[üßä point events]\n",
    "    E3[üßä state events]\n",
    "\n",
    "    F1[üßä data]\n",
    "    F2[üßä point events]\n",
    "    F3[üßä state events]\n",
    "\n",
    "    %% Connections\n",
    "    A --> D1\n",
    "    A --> D2\n",
    "    A --> D3\n",
    "\n",
    "    B --> E1\n",
    "    B --> E2\n",
    "    B --> E3\n",
    "\n",
    "    C --> F1\n",
    "    C --> F2\n",
    "    C --> F3\n",
    "```\n",
    "(Legend: üõ†Ô∏è = project. üßä = iceberg.)\n",
    "\n",
    "To see all of the views available, use the `list_dataset_views` on your DuckPond instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "import DiveDB.services.duck_pond\n",
    "importlib.reload(DiveDB.services.duck_pond)\n",
    "\n",
    "from DiveDB.services.duck_pond import DuckPond\n",
    "\n",
    "duck_pond = DuckPond(os.environ[\"CONTAINER_ICEBERG_PATH\"])\n",
    "\n",
    "display(duck_pond.list_all_views())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main ways to query data from the Iceberg data lake:\n",
    "1. Using the DuckPond `get_data` method\n",
    "2. Using the DuckPond connection to query directly\n",
    "\n",
    "### Using the DuckPond `get_data` method\n",
    "DuckPond's `get_data` method constructs a query based on the parameters you pass to it and returns a DuckDB DataFrame. It takes the following optional parameters:\n",
    "- `labels`: A string or list of data labels to query.\n",
    "- `logger_ids`: A string or list of logger IDs to query.\n",
    "- `animal_ids`: A string or list of animal IDs to query.\n",
    "- `deployment_ids`: A string or list of deployment IDs to query.\n",
    "- `recording_ids`: A string or list of recording IDs to query.\n",
    "- `date_range`: A tuple of start and end dates to query.\n",
    "- `limit`: The maximum number of rows to return.\n",
    "\n",
    "The `get_data` method returns a [DuckDB DuckDBPyConnection](https://duckdb.org/docs/api/python/reference/#duckdb.DuckDBPyConnection) which can be used to convert the data in many different formats including the following ([see documentation for a full list](https://duckdb.org/docs/api/python/conversion#result-conversion-duckdb-results-to-python))\n",
    "- NumPy Array (`.fetchnumpy()`)\n",
    "- Pandas DataFrame (`.df()`)\n",
    "- Arrows Table (`.arrow()`)\n",
    "- Polars DataFrame (`.pl()`)\n",
    "\n",
    "Until a conversion method is called, the data is not loaded into memory. This allows for large queries to be run without using too much memory.\n",
    "\n",
    "##### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "import DiveDB.services.duck_pond\n",
    "importlib.reload(DiveDB.services.duck_pond)\n",
    "\n",
    "from DiveDB.services.duck_pond import DuckPond\n",
    "\n",
    "duck_pond = DuckPond(os.environ[\"CONTAINER_ICEBERG_PATH\"])\n",
    "\n",
    "conn = duck_pond.conn.sql(\"\"\"SELECT count(*) FROM \"EP Physiology_Data\" \"\"\")\n",
    "\n",
    "display(conn.df())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying for shared frequency from Iceberg\n",
    "Iceberg can store multiple signal names at a single frequency. If you query a single signal name, the data will be returned as a list of values for each timestamp. If you query multiple signal names, the data will be returned as a list of lists of values for each timestamp.\n",
    "\n",
    "The data will be returned as a Pandas DataFrame with a DatetimeIndex.\n",
    "\n",
    "##### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "import DiveDB.services.duck_pond\n",
    "importlib.reload(DiveDB.services.duck_pond)\n",
    "\n",
    "from DiveDB.services.duck_pond import DuckPond\n",
    "\n",
    "duck_pond = DuckPond(os.environ[\"CONTAINER_ICEBERG_PATH\"])\n",
    "\n",
    "duck_pond.get_data(    \n",
    "    labels=[\"derived_data_depth\"],\n",
    "    animal_ids=\"apfo-001a\",\n",
    "    dataset=\"EP Physiology\",\n",
    "    frequency=1/60,  # Once a minute\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the IcePond connection to query directly\n",
    "More complex queries can be run directly on the DuckPond connection. This is useful for queries that may not be supported by the `get_data` method which has those involving grouping or aggregations. \n",
    "\n",
    "DuckDB runs sql very similar in syntax to other SQL databases. A full breakdown of the syntax can be found [in the documenation](https://duckdb.org/docs/sql/introduction).\n",
    "\n",
    "The connection object can be found in the `duck_pond.conn` attribute. To run queries, use the `sql` method which also returns a [DuckDB DuckDBPyConnection](https://duckdb.org/docs/api/python/reference/#duckdb.DuckDBPyConnection) which can be used to convert the data in many different formats including the following ([see documentation for a full list](https://duckdb.org/docs/api/python/conversion#result-conversion-duckdb-results-to-python))\n",
    "- NumPy Array (`.fetchnumpy()`)\n",
    "- Pandas DataFrame (`.df()`)\n",
    "- Arrows Table (`.arrow()`)\n",
    "- Polars DataFrame (`.pl()`)\n",
    "\n",
    "##### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "# Reload the DuckPond module to pick up any changes\n",
    "import DiveDB.services.duck_pond\n",
    "importlib.reload(DiveDB.services.duck_pond)\n",
    "from DiveDB.services.duck_pond import DuckPond\n",
    "\n",
    "duck_pond = DuckPond(os.environ[\"CONTAINER_ICEBERG_PATH\"])\n",
    "\n",
    "df = duck_pond.conn.sql(f\"\"\"\n",
    "    SELECT label, avg(float_value) as mean_data\n",
    "    FROM \"EP Physiology_Data\"\n",
    "    WHERE label = 'sensor_data_light'\n",
    "    OR label = 'sensor_data_temperature'\n",
    "    GROUP BY label\n",
    "\"\"\").df()\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining Queries\n",
    "Queries can be chained together to form a pipeline. This is useful for running complex queries that involve multiple steps.\n",
    "\n",
    "##### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "# Reload the DuckPond module to pick up any changes\n",
    "import DiveDB.services.duck_pond\n",
    "importlib.reload(DiveDB.services.duck_pond)\n",
    "from DiveDB.services.duck_pond import DuckPond\n",
    "\n",
    "duck_pond = DuckPond(os.environ[\"CONTAINER_ICEBERG_PATH\"])\n",
    "\n",
    "# Get the filtered data\n",
    "filtered_data = duck_pond.get_data(    \n",
    "    dataset=\"EP Physiology\",\n",
    "    # Resample values to 10 Hz and make sure each signal has the same time intervals\n",
    "    frequency=10,\n",
    "    # Aggregation of events (think state events - behaviors) type: state (has state and end dates)\n",
    "    classes=\"sensor_data_accelerometer\",\n",
    "    \n",
    ")\n",
    "\n",
    "display(filtered_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Variables\n",
    "Sometimes we don't want to hardcode variables in our queries. We can use the `execute` method to pass variables to the query.\n",
    "\n",
    "##### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "# Reload the DuckPond module to pick up any changes\n",
    "import DiveDB.services.duck_pond\n",
    "importlib.reload(DiveDB.services.duck_pond)\n",
    "from DiveDB.services.duck_pond import DuckPond\n",
    "\n",
    "duck_pond = DuckPond(os.environ[\"CONTAINER_ICEBERG_PATH\"])\n",
    "\n",
    "label = \"sensor_data_temperature\"\n",
    "df = duck_pond.conn.execute(f\"\"\"\n",
    "    SELECT label, avg(float_value) as mean_data\n",
    "    FROM \"EP Physiology_Data\"\n",
    "    WHERE label = $1\n",
    "    GROUP BY label\n",
    "\"\"\", [label]).df()\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Metadata Database\n",
    "We can also query the Metadata Database directly. This is useful for querying data that is not stored in Delta Lake and joining it for queries on measurement data.\n",
    "\n",
    "##### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "# Reload the DuckPond module to pick up any changes\n",
    "import DiveDB.services.duck_pond\n",
    "importlib.reload(DiveDB.services.duck_pond)\n",
    "from DiveDB.services.duck_pond import DuckPond\n",
    "\n",
    "from DiveDB.services.notion_orm import NotionORMManager\n",
    "\n",
    "notion_manager = NotionORMManager(\n",
    "    db_map={\"Animal DB\": os.environ[\"ANIMALS_DB_ID\"]},\n",
    "    token=os.environ[\"NOTION_API_KEY\"],\n",
    ")\n",
    "\n",
    "duck_pond = DuckPond(os.environ[\"CONTAINER_ICEBERG_PATH\"], notion_manager=notion_manager)\n",
    "\n",
    "\n",
    "# Show all tables we have access to\n",
    "print(duck_pond.get_db_schema())\n",
    "\n",
    "df = duck_pond.conn.sql(\"\"\"\n",
    "    SELECT * FROM Animals LIMIT 10\n",
    "\"\"\").df()\n",
    "\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Data to EDF\n",
    "When it's easier to work with EDF files, we can export the data to an EDF file. This is useful for working with the data in other software packages.\n",
    "\n",
    "Calling `export_to_edf(output_dir)` on a `DiveData` object creates one output EDF file for each recording in the `DiveData` relation, saved to `output_dir` with filename `<recording_id>.edf`. \n",
    "\n",
    "*Note: it currently requires a lot of memory. Can be improved.*<br/>\n",
    "*Note: it's lacking support for most info fields in the EDF file.*\n",
    "\n",
    "##### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "import DiveDB.services.duck_pond\n",
    "import DiveDB.services.dive_data\n",
    "importlib.reload(DiveDB.services.duck_pond)\n",
    "importlib.reload(DiveDB.services.dive_data)\n",
    "\n",
    "from DiveDB.services.duck_pond import DuckPond\n",
    "from DiveDB.services.notion_orm import NotionORMManager\n",
    "\n",
    "notion_manager = NotionORMManager(\n",
    "    db_map={\"Animal DB\": os.environ[\"ANIMALS_DB_ID\"]},\n",
    "    token=os.environ[\"NOTION_API_KEY\"],\n",
    ")\n",
    "\n",
    "duck_pond = DuckPond(os.environ[\"CONTAINER_ICEBERG_PATH\"], notion_manager=notion_manager)\n",
    "\n",
    "dive_data = duck_pond.get_data(    \n",
    "    dataset=\"EP Physiology\",\n",
    "    labels=[\"sensor_data_temperature\", \"derived_data_depth\"],\n",
    "    limit=1000000,\n",
    ")\n",
    "\n",
    "output_edf_paths = dive_data.export_to_edf(\".tmp/my_output_dir/\")\n",
    "display(output_edf_paths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing exported EDF as an MNE Signal Array\n",
    "For working with the data in MNE, we can export the data to an EDF and then import it to MNE.\n",
    "\n",
    "\n",
    "##### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import importlib\n",
    "import os\n",
    "import DiveDB.services.duck_pond\n",
    "importlib.reload(DiveDB.services.duck_pond)\n",
    "from DiveDB.services.duck_pond import DuckPond\n",
    "from DiveDB.services.notion_orm import NotionORMManager\n",
    "\n",
    "notion_manager = NotionORMManager(\n",
    "    db_map={\"Animal DB\": os.environ[\"ANIMALS_DB_ID\"]},\n",
    "    token=os.environ[\"NOTION_API_KEY\"],\n",
    ")\n",
    "\n",
    "duck_pond = DuckPond(os.environ[\"CONTAINER_ICEBERG_PATH\"], notion_manager=notion_manager)\n",
    "\n",
    "dive_data = duck_pond.get_data(    \n",
    "    dataset=\"EP Physiology\",\n",
    "    labels=\"ECG_ICA2\",\n",
    "    limit=1000000,\n",
    ")\n",
    "\n",
    "output_edf_paths = dive_data.export_to_edf(\".tmp/my_output_dir/\")\n",
    "raw = mne.io.read_raw_edf(output_edf_paths[0])\n",
    "display(raw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DiveDB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
