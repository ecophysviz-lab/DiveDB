{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export EDF\n",
    "\n",
    "This notebook demonstrates the process of exporting DiveDB data as an EDF file.\n",
    "\n",
    "While under development, it also contains the prototype (non-library) code; that'll be deleted when this notebook is ready to be merged into the main branch.\n",
    "\n",
    "Punch list:\n",
    "- [x] Make a list\n",
    "- [x] Understand task :) \n",
    "- [ ] Prototype:\n",
    "    - [x] Load basic metadata\n",
    "    - [x] Load signals\n",
    "    - [x] Generate EDF file \n",
    "        - [X] Can mne serve our needs here? Check if multiple sample rates, arbitrary metadata: edfio can!\n",
    "        - [x] Decide if different library OR extend mne: use edfio, which is what mne depends on \n",
    "    - [x] Test EDF file can be opened externally (e.g. through EDF.jl or other app)\n",
    "    - [x] Test EDF encodes max/min values\n",
    "    - [ ] Add metadata to EDF header\n",
    "- [ ] In tests, write (failing) test for basic new functionality\n",
    "- [ ] Turn prototype into library code - test passes!\n",
    "- [ ] Write up edge case tests\n",
    "    - [ ] Make 'em pass OR file 'em\n",
    "- [ ] Clean up this notebook (delete this punch list!)\n",
    "- [ ] Mark PR ready for review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder: this is the end goal\n",
    "\n",
    "```python\n",
    "# Example of usage once complete\n",
    "\n",
    "from DiveDB.services.duck_pond import DuckPond\n",
    "\n",
    "duckpond = DuckPond(os.environ[\"CONTAINER_DELTA_LAKE_PATH\"])\n",
    "\n",
    "dive_data = duckpond.get_delta_data(    \n",
    "    labels=[\"eeg\"],\n",
    "    animal_ids=\"apfo-001a\",\n",
    ")\n",
    "\n",
    "dive_data.export_to_edf(\"path_to_output.edf\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>derived_data_depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-11-07 19:50:00+00:00</td>\n",
       "      <td>-1.982753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-07 19:51:00+00:00</td>\n",
       "      <td>-1.900138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-11-07 19:52:00+00:00</td>\n",
       "      <td>-1.660804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-11-07 19:53:00+00:00</td>\n",
       "      <td>-1.362733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-11-07 19:54:00+00:00</td>\n",
       "      <td>-1.074403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>2019-11-07 22:43:00+00:00</td>\n",
       "      <td>-0.466027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>2019-11-07 22:44:00+00:00</td>\n",
       "      <td>-0.794010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>2019-11-07 22:45:00+00:00</td>\n",
       "      <td>-1.119214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>2019-11-07 22:46:00+00:00</td>\n",
       "      <td>-1.418781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>2019-11-07 22:47:00+00:00</td>\n",
       "      <td>-1.611978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     datetime  derived_data_depth\n",
       "0   2019-11-07 19:50:00+00:00           -1.982753\n",
       "1   2019-11-07 19:51:00+00:00           -1.900138\n",
       "2   2019-11-07 19:52:00+00:00           -1.660804\n",
       "3   2019-11-07 19:53:00+00:00           -1.362733\n",
       "4   2019-11-07 19:54:00+00:00           -1.074403\n",
       "..                        ...                 ...\n",
       "173 2019-11-07 22:43:00+00:00           -0.466027\n",
       "174 2019-11-07 22:44:00+00:00           -0.794010\n",
       "175 2019-11-07 22:45:00+00:00           -1.119214\n",
       "176 2019-11-07 22:46:00+00:00           -1.418781\n",
       "177 2019-11-07 22:47:00+00:00           -1.611978\n",
       "\n",
       "[178 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┌──────────┬─────────┬────────────────────────────┬─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┬──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┬───────────┐\n",
       "│ database │ schema  │            name            │                                                                                                                                                          column_names                                                                                                                                                           │                                                                                                           column_types                                                                                                           │ temporary │\n",
       "│ varchar  │ varchar │          varchar           │                                                                                                                                                            varchar[]                                                                                                                                                            │                                                                                                            varchar[]                                                                                                             │  boolean  │\n",
       "├──────────┼─────────┼────────────────────────────┼─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼───────────┤\n",
       "│ memory   │ main    │ DataLake                   │ [animal, deployment, recording, group, class, label, datetime, value]                                                                                                                                                                                                                                                           │ [VARCHAR, VARCHAR, VARCHAR, VARCHAR, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, STRUCT(\"float\" DOUBLE, string VARCHAR, \"boolean\" BOOLEAN, \"int\" BIGINT)]                                                                        │ false     │\n",
       "│ metadata │ public  │ Animal_Deployments         │ [id, animal_id, deployment_id]                                                                                                                                                                                                                                                                                                  │ [BIGINT, VARCHAR, VARCHAR]                                                                                                                                                                                                       │ false     │\n",
       "│ metadata │ public  │ Animals                    │ [id, project_id, common_name, scientific_name, birth_year, domain_ids, lab_id, sex]                                                                                                                                                                                                                                             │ [VARCHAR, VARCHAR, VARCHAR, VARCHAR, INTEGER, VARCHAR, VARCHAR, VARCHAR]                                                                                                                                                         │ false     │\n",
       "│ metadata │ public  │ Deployments                │ [id, rec_date, animal, start_time, start_time_precision, timezone, notes, deployment_name, animal_age, animal_age_class, arrival_datetime, departure_datetime, deployment_latitude, deployment_location, deployment_longitude, deployment_type, domain_deployment_id, recovery_latitude, recovery_location, recovery_longitude] │ [VARCHAR, DATE, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR, VARCHAR, INTEGER, VARCHAR, TIMESTAMP WITH TIME ZONE, TIMESTAMP WITH TIME ZONE, DOUBLE, VARCHAR, DOUBLE, VARCHAR, VARCHAR, DOUBLE, VARCHAR, DOUBLE] │ false     │\n",
       "│ metadata │ public  │ Files                      │ [id, extension, type, delta_path, recording_id, metadata, start_time, uploaded_at, file]                                                                                                                                                                                                                                        │ [BIGINT, VARCHAR, VARCHAR, VARCHAR, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, TIMESTAMP WITH TIME ZONE, VARCHAR]                                                                                                               │ false     │\n",
       "│ metadata │ public  │ Logger_Wikis               │ [id, description, tags, projects]                                                                                                                                                                                                                                                                                               │ [BIGINT, VARCHAR, VARCHAR[], VARCHAR[]]                                                                                                                                                                                          │ false     │\n",
       "│ metadata │ public  │ Loggers                    │ [id, icon_url, serial_no, manufacturer, type, type_name, notes, owner, wiki_id, manufacturer_name, ptt]                                                                                                                                                                                                                         │ [VARCHAR, VARCHAR, VARCHAR, VARCHAR, VARCHAR, VARCHAR, VARCHAR, VARCHAR, BIGINT, VARCHAR, VARCHAR]                                                                                                                               │ false     │\n",
       "│ metadata │ public  │ Media_Updates              │ [id, update_type, update_factor, file_id]                                                                                                                                                                                                                                                                                       │ [BIGINT, VARCHAR, DOUBLE, BIGINT]                                                                                                                                                                                                │ false     │\n",
       "│ metadata │ public  │ Recordings                 │ [id, start_time, actual_start_time, end_time, start_time_precision, animal_deployment_id, logger_id, name, attachment_location, attachment_type, quality, timezone]                                                                                                                                                             │ [VARCHAR, TIMESTAMP WITH TIME ZONE, TIMESTAMP WITH TIME ZONE, TIMESTAMP WITH TIME ZONE, VARCHAR, BIGINT, VARCHAR, VARCHAR, VARCHAR, VARCHAR, VARCHAR, VARCHAR]                                                                   │ false     │\n",
       "│ metadata │ public  │ auth_group                 │ [id, name]                                                                                                                                                                                                                                                                                                                      │ [INTEGER, VARCHAR]                                                                                                                                                                                                               │ false     │\n",
       "│ metadata │ public  │ auth_group_permissions     │ [id, group_id, permission_id]                                                                                                                                                                                                                                                                                                   │ [BIGINT, INTEGER, INTEGER]                                                                                                                                                                                                       │ false     │\n",
       "│ metadata │ public  │ auth_permission            │ [id, name, content_type_id, codename]                                                                                                                                                                                                                                                                                           │ [INTEGER, VARCHAR, INTEGER, VARCHAR]                                                                                                                                                                                             │ false     │\n",
       "│ metadata │ public  │ auth_user                  │ [id, password, last_login, is_superuser, username, first_name, last_name, email, is_staff, is_active, date_joined]                                                                                                                                                                                                              │ [INTEGER, VARCHAR, TIMESTAMP WITH TIME ZONE, BOOLEAN, VARCHAR, VARCHAR, VARCHAR, VARCHAR, BOOLEAN, BOOLEAN, TIMESTAMP WITH TIME ZONE]                                                                                            │ false     │\n",
       "│ metadata │ public  │ auth_user_groups           │ [id, user_id, group_id]                                                                                                                                                                                                                                                                                                         │ [BIGINT, INTEGER, INTEGER]                                                                                                                                                                                                       │ false     │\n",
       "│ metadata │ public  │ auth_user_user_permissions │ [id, user_id, permission_id]                                                                                                                                                                                                                                                                                                    │ [BIGINT, INTEGER, INTEGER]                                                                                                                                                                                                       │ false     │\n",
       "│ metadata │ public  │ django_admin_log           │ [id, action_time, object_id, object_repr, action_flag, change_message, content_type_id, user_id]                                                                                                                                                                                                                                │ [INTEGER, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, SMALLINT, VARCHAR, INTEGER, INTEGER]                                                                                                                                       │ false     │\n",
       "│ metadata │ public  │ django_content_type        │ [id, app_label, model]                                                                                                                                                                                                                                                                                                          │ [INTEGER, VARCHAR, VARCHAR]                                                                                                                                                                                                      │ false     │\n",
       "│ metadata │ public  │ django_migrations          │ [id, app, name, applied]                                                                                                                                                                                                                                                                                                        │ [BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE]                                                                                                                                                                             │ false     │\n",
       "│ metadata │ public  │ django_session             │ [session_key, session_data, expire_date]                                                                                                                                                                                                                                                                                        │ [VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE]                                                                                                                                                                                     │ false     │\n",
       "│ temp     │ main    │ pivot_results              │ [datetime, derived_data_depth_float, derived_data_depth_int, derived_data_depth_bool, derived_data_depth_str]                                                                                                                                                                                                                   │ [TIMESTAMP WITH TIME ZONE, DOUBLE, BIGINT, BOOLEAN, VARCHAR]                                                                                                                                                                     │ true      │\n",
       "├──────────┴─────────┴────────────────────────────┴─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴───────────┤\n",
       "│ 20 rows                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                6 columns │\n",
       "└──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Get metadata\n",
    "import os\n",
    "import importlib\n",
    "import DiveDB.services.duck_pond as dp\n",
    "importlib.reload(dp)\n",
    "\n",
    "duckpond = dp.DuckPond(os.environ[\"CONTAINER_DELTA_LAKE_PATH\"])\n",
    "\n",
    "# Example from the querying_docs notebook\n",
    "data = duckpond.get_delta_data(    \n",
    "    labels=[\"derived_data_depth\"],\n",
    "    animal_ids=\"apfo-001a\",\n",
    "    frequency=1/60,  # Once a minute\n",
    ")\n",
    "display(data)\n",
    "\n",
    "# Okay, but is there a way to find out what animal_ids, etc, are available?\n",
    "# Time to go spelunking!\n",
    "duckpond.get_db_schema()\n",
    "\n",
    "# ...okay, cool. :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>derived_data_corrected_acc</td>\n",
       "      <td>az</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logger_data_CC-35</td>\n",
       "      <td>light2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logger_data_CC-35</td>\n",
       "      <td>gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>derived_data_corrected_gyr</td>\n",
       "      <td>gy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sensor_data_magnetometer</td>\n",
       "      <td>mz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>logger_data_CC-35</td>\n",
       "      <td>my</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>logger_data_CC-35</td>\n",
       "      <td>az</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>sensor_data_gyroscope</td>\n",
       "      <td>gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>derived_data_calibrated_mag</td>\n",
       "      <td>mz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>logger_data_CC-35</td>\n",
       "      <td>mx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          class   label\n",
       "0    derived_data_corrected_acc      az\n",
       "1             logger_data_CC-35  light2\n",
       "2             logger_data_CC-35      gz\n",
       "3    derived_data_corrected_gyr      gy\n",
       "4      sensor_data_magnetometer      mz\n",
       "..                          ...     ...\n",
       "56            logger_data_CC-35      my\n",
       "57            logger_data_CC-35      az\n",
       "58        sensor_data_gyroscope      gz\n",
       "59  derived_data_calibrated_mag      mz\n",
       "60            logger_data_CC-35      mx\n",
       "\n",
       "[61 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's try a sql query as well (also ripped from the querying_docs notebook)\n",
    "labels_df = duckpond.conn.sql(f\"\"\"\n",
    "    SELECT label\n",
    "    FROM (\n",
    "        SELECT DISTINCT label\n",
    "        FROM DataLake\n",
    "    )\n",
    "\"\"\").df()\n",
    "# display(labels_df)\n",
    "\n",
    "animals_df = duckpond.conn.sql(f\"\"\"\n",
    "    SELECT animal\n",
    "    FROM (\n",
    "        SELECT DISTINCT animal\n",
    "        FROM DataLake\n",
    "    )\n",
    "\"\"\").df()\n",
    "# display(animals_df)\n",
    "\n",
    "signal_df = duckpond.conn.sql(f\"\"\"\n",
    "    SELECT class, label\n",
    "    FROM (\n",
    "        SELECT DISTINCT label, class\n",
    "        FROM DataLake\n",
    "    )\n",
    "\"\"\").df()\n",
    "display(signal_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>derived_data_calibrated_acc</td>\n",
       "      <td>ax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>derived_data_calibrated_acc</td>\n",
       "      <td>ay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>derived_data_calibrated_acc</td>\n",
       "      <td>az</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>derived_data_calibrated_mag</td>\n",
       "      <td>mz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>derived_data_calibrated_mag</td>\n",
       "      <td>mx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sensor_data_magnetometer</td>\n",
       "      <td>mz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>sensor_data_magnetometer</td>\n",
       "      <td>my</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>sensor_data_magnetometer</td>\n",
       "      <td>mx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>sensor_data_pressure</td>\n",
       "      <td>sensor_data_pressure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>sensor_data_temperature</td>\n",
       "      <td>sensor_data_temperature</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          class                    label\n",
       "52  derived_data_calibrated_acc                       ax\n",
       "45  derived_data_calibrated_acc                       ay\n",
       "25  derived_data_calibrated_acc                       az\n",
       "59  derived_data_calibrated_mag                       mz\n",
       "49  derived_data_calibrated_mag                       mx\n",
       "..                          ...                      ...\n",
       "4      sensor_data_magnetometer                       mz\n",
       "41     sensor_data_magnetometer                       my\n",
       "39     sensor_data_magnetometer                       mx\n",
       "29         sensor_data_pressure     sensor_data_pressure\n",
       "40      sensor_data_temperature  sensor_data_temperature\n",
       "\n",
       "[61 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_df.sort_values(by=\"class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commenting out b/c otherwise this crashes my kernel (if i do other stuff after it)\n",
    "\n",
    "# # Once more from the other notebook....\n",
    "# # Get the filtered data\n",
    "# resampled_data = duckpond.get_delta_data(    \n",
    "#     animal_ids=\"apfo-001a\",\n",
    "#     # Resample values to 100 Hz and make sure each signal has the same time intervals\n",
    "#     frequency=100,\n",
    "#     # Aggregation of events (think state events - behaviors) type: state (has state and end dates)\n",
    "#     classes=\"sensor_data_accelerometer\",\n",
    "# )\n",
    "# display(resampled_data)\n",
    "# # Huh. okay, `frequency` triggering a materialization + resample is interesting, not sure \n",
    "# # I would have guessed that from the API! I would have guessed that had to do with \n",
    "# # the sampling rate of the recording.\n",
    "\n",
    "# # Okay, so the output of `get_delta_data` with a set frequency returns the signal as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f5e861152c4f62a05f68c0ef922b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┌─────────────────────────────┬───────────────┬────────────────┬───────────────┐\n",
       "│          datetime           │      ax       │       az       │      ay       │\n",
       "│  timestamp with time zone   │    double     │     double     │    double     │\n",
       "├─────────────────────────────┼───────────────┼────────────────┼───────────────┤\n",
       "│ 2019-11-07 19:50:45+00      │ -0.0071826049 │ -10.6901104125 │  0.0263362182 │\n",
       "│ 2019-11-07 19:50:45.0025+00 │  0.0167594116 │  -10.637437976 │  0.0407014282 │\n",
       "│ 2019-11-07 19:50:45.005+00  │ -0.0167594116 │ -10.6565915893 │  0.0430956298 │\n",
       "│ 2019-11-07 19:50:45.0075+00 │  0.0239420166 │ -10.7356002441 │  0.0430956298 │\n",
       "│ 2019-11-07 19:50:45.01+00   │  0.0023942016 │ -10.6158901611 │  0.0478840332 │\n",
       "│ 2019-11-07 19:50:45.0125+00 │  0.0023942016 │ -10.7236292358 │  0.0622492431 │\n",
       "│ 2019-11-07 19:50:45.015+00  │  0.0526724365 │ -10.6637741943 │  0.0287304199 │\n",
       "│ 2019-11-07 19:50:45.0175+00 │  0.0071826049 │ -10.6565915893 │  0.0454898315 │\n",
       "│ 2019-11-07 19:50:45.02+00   │  0.0119710083 │  -10.644620581 │  0.0454898315 │\n",
       "│ 2019-11-07 19:50:45.0225+00 │ -0.0047884033 │ -10.6733510009 │  0.0885854614 │\n",
       "│             ·               │        ·      │        ·       │        ·      │\n",
       "│             ·               │        ·      │        ·       │        ·      │\n",
       "│             ·               │        ·      │        ·       │        ·      │\n",
       "│ 2019-11-07 19:51:09.975+00  │  0.9888052856 │  -10.292672937 │ -0.6177040283 │\n",
       "│ 2019-11-07 19:51:09.9775+00 │  0.9768342773 │ -10.2040874755 │ -0.6464344482 │\n",
       "│ 2019-11-07 19:51:09.98+00   │  0.9217676391 │ -10.2232410888 │ -0.6368576416 │\n",
       "│ 2019-11-07 19:51:09.9825+00 │  0.9672574707 │ -10.2950671386 │ -0.6344634399 │\n",
       "│ 2019-11-07 19:51:09.985+00  │  0.9457096557 │  -10.165780249 │ -0.5961562133 │\n",
       "│ 2019-11-07 19:51:09.9875+00 │  1.0414777221 │  -10.151415039 │ -0.5291185668 │\n",
       "│ 2019-11-07 19:51:09.99+00   │  0.9816226806 │  -10.151415039 │ -0.4596867187 │\n",
       "│ 2019-11-07 19:51:09.9925+00 │  0.9864110839 │ -10.2447889038 │ -0.4836287353 │\n",
       "│ 2019-11-07 19:51:09.995+00  │  1.0295067138 │ -10.2304236938 │ -0.4405331054 │\n",
       "│ 2019-11-07 19:51:09.9975+00 │   1.077390747 │ -10.1346556274 │ -0.4548983154 │\n",
       "├─────────────────────────────┴───────────────┴────────────────┴───────────────┤\n",
       "│ ? rows (>9999 rows, 20 shown)                                      4 columns │\n",
       "└──────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Is there a way to get the original sample rate? \n",
    "unmaterialized_data = duckpond.get_delta_data(    \n",
    "    animal_ids=\"apfo-001a\",\n",
    "    # Resample values to 10 Hz and make sure each signal has the same time intervals\n",
    "    frequency=None,\n",
    "    # Aggregation of events (think state events - behaviors) type: state (has state and end dates)\n",
    "    classes=\"sensor_data_accelerometer\",\n",
    ")\n",
    "display(unmaterialized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'duckdb.duckdb.DuckDBPyRelation'>\n"
     ]
    }
   ],
   "source": [
    "# ... okay, got it. now, let's do what needs doing. \n",
    "# But also, keep in mind that we should NOT pass a frequency into `get_delta_data`\n",
    "# before EDF export unless we are very explicit about what we are doing and why. \n",
    "\n",
    "# When we don't pass in a frequency (i.e., resample), we get a DuckDBPyRelation\n",
    "# out of `get_delta_data`\n",
    "print(type(unmaterialized_data))\n",
    "\n",
    "# ...from task, I think we want a DuckDBPyConnection instead? Currently unclear to me\n",
    "# how these interop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<EdfSignal EEG Fpz 256Hz>, <EdfSignal Body Temp 1Hz>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.96627737, -0.16337485,  0.20449128, ...,  0.51771356,\n",
       "        0.59518333, -2.06714838])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Okay, now to an EDF! \n",
    "# Let's do the demo from edfio (what mne depends on for its EDF support)\n",
    "\n",
    "from edfio import Edf, EdfSignal, read_edf\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# edfio's example\n",
    "example_edf = Edf(\n",
    "    [\n",
    "        EdfSignal(np.random.randn(30 * 256), sampling_frequency=256, label=\"EEG Fpz\"),\n",
    "        EdfSignal(np.random.randn(30), sampling_frequency=1, label=\"Body Temp\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "outpath = \".tmp/example.edf\"\n",
    "example_edf.write(outpath)\n",
    "\n",
    "example_edf_roundtrip = read_edf(outpath)\n",
    "display(example_edf_roundtrip.signals)\n",
    "display(example_edf_roundtrip.signals[0].data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66c5a6d1f4d47d0b3eb86d77a1be2df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ...and now with our data!\n",
    "# Can we make an EDF from our data? \n",
    "# intentionally picking signals with different sampling rates\n",
    "\n",
    "# ...normally we could query these all at the same time, except that we're putting limits\n",
    "# on here so that we don't have to get ALL values for each signal. Also, in real case, \n",
    "# this is where we'd pull all data and then split it up and make one EDF file per animal/deployment/etc. \n",
    "# For now? Hard code it, bebe!\n",
    "signals = []\n",
    "classes = [\"sensor_data_accelerometer\",\"sensor_data_pressure\"]\n",
    "\n",
    "sig_connections = []\n",
    "max_duration_sec = 0\n",
    "\n",
    "# Set up for EDF - first figure out what common max duration is, etc\n",
    "for class_name in classes:\n",
    "    df_raw = duckpond.get_delta_data(    \n",
    "        animal_ids=\"apfo-001a\",\n",
    "        classes=[class_name],\n",
    "        # limit=1000,\n",
    "    )\n",
    "\n",
    "    #TODO-optimize: surely there is a way to get the number of samples without loading them all?? \n",
    "    # If so, do that!\n",
    "    sampling_rate = df[\"datetime\"].diff()[1:].dt.total_seconds().unique()[0]\n",
    "    sampling_frequency = int(1/sampling_rate) # TODO-safety: don't just blindly round o_O\n",
    "\n",
    "    # TODO-correctly! need to figure out max signal length, then start time, then \n",
    "    # Lpad to the correct start time + lpad to the correct stop time (lol EDF)\n",
    "    # For now, we're faking it. We happen to know that the maximum duration signal \n",
    "    # of these two is 20 s, so lets zero-pad to that:\n",
    "    sig_max_duration_sec = math.ceil(df_raw.shape[0] / sampling_frequency)\n",
    "    max_duration_sec = max(max_duration_sec, sig_max_duration_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36adbece0dbc452380f11939acecdeaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'ax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ax'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[151], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m num_channels \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     39\u001b[0m signal_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((max_duration_sec \u001b[38;5;241m*\u001b[39m sampling_frequency, num_channels), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m---> 40\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchannel_label\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, num_channels):\n\u001b[1;32m     42\u001b[0m     channel_label \u001b[38;5;241m=\u001b[39m (df\u001b[38;5;241m.\u001b[39mcolumns)[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m#TODO: save channel labels\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ax'"
     ]
    }
   ],
   "source": [
    "# Now actually use the real data to create an edf\n",
    "for class_name in classes:\n",
    "    df = duckpond.get_delta_data(    \n",
    "        animal_ids=\"apfo-001a\",\n",
    "        classes=[class_name],\n",
    "        # limit=1000,\n",
    "    ).df()\n",
    "\n",
    "    # TODO-safety: check that there's only one value after the unique, check that \n",
    "    # this is an integer value or whatever the EDF spec requires, etc\n",
    "    sampling_rate = df[\"datetime\"].diff()[1:].dt.total_seconds().unique()[0]\n",
    "    sampling_frequency = int(1/sampling_rate) # TODO-safety: don't just blindly round, see if we allow floating point values here also?? o_O\n",
    "\n",
    "    signal_label = class_name if len(class_name) <= 16 else class_name[0:16] # lol EDF\n",
    "\n",
    "    # Need to figure out max signal length, then start time, then \n",
    "    # Lpad to the correct start time + lpad to the correct stop time (lol EDF)\n",
    "    # TODO-future: instead of padding w/ 0, use some signal-specific value\n",
    "    num_channels = df.shape[1] - 1\n",
    "    signal_data = np.zeros((max_duration_sec * sampling_frequency, num_channels), dtype=np.float64)\n",
    "    for i in range(0, num_channels):\n",
    "        channel_label = (df.columns)[i + 1] #TODO: save channel labels\n",
    "        num_samples = len(df[channel_label].values)\n",
    "        signal_data[0:num_samples, i] = df[channel_label].values\n",
    "\n",
    "    signal = EdfSignal(signal_data,\n",
    "                        sampling_frequency=sampling_frequency, \n",
    "                        label=signal_label)\n",
    "\n",
    "    signals.append(signal)\n",
    "    \n",
    "divedb_edf = Edf(signals)\n",
    "path = \".tmp/prototype.edf\"\n",
    "divedb_edf.write(path)\n",
    "\n",
    "divedb_edf_roundtrip = read_edf(path)\n",
    "display(divedb_edf_roundtrip.signals)\n",
    "display(divedb_edf_roundtrip.signals[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c8dfd0b9ae4bf7860cecfe76938509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_name = \"sensor_data_accelerometer\"\n",
    "df = duckpond.get_delta_data(    \n",
    "    animal_ids=\"apfo-001a\",\n",
    "    classes=[class_name],\n",
    "    # limit=1000,\n",
    ").df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO-safety: check that there's only one value after the unique, check that \n",
    "# this is an integer value or whatever the EDF spec requires, etc\n",
    "sampling_rate = df[\"datetime\"].diff()[1:].dt.total_seconds().unique()[0]\n",
    "sampling_frequency = int(1/sampling_rate) # TODO-safety: don't just blindly round o_O\n",
    "\n",
    "label_sanitized = label if len(label) <= 16 else label[0:16] # lol EDF\n",
    "\n",
    "# TODO-correctly! need to figure out max signal length, then start time, then \n",
    "# Lpad to the correct start time + lpad to the correct stop time (lol EDF)\n",
    "# For now, we're faking it. We happen to know that the maximum duration signal \n",
    "# of these two is 20 s, so lets zero-pad to that:\n",
    "max_duration_sec = math.ceil(df.shape[0] / sampling_frequency) # Find over all input signals\n",
    "num_channels = df.shape[1] - 1\n",
    "signal_data = np.zeros((max_duration_sec * sampling_frequency, num_channels), dtype=np.float64)\n",
    "num_samples = len(df[channel_label].values)\n",
    "for i in range(0, num_channels):\n",
    "    channel_label = (df.columns)[i + 1]\n",
    "    num_samples = len(df[channel_label].values)\n",
    "    signal_data[0:num_samples, i] = df[channel_label].values\n",
    "\n",
    "signal = EdfSignal(signal_data,\n",
    "                    sampling_frequency=sampling_frequency, \n",
    "                    label=label_sanitized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commenting out to avoid an unnecessary error! Uncomment to see the (fully expected) error. :) \n",
    "# # Can an edf contain nan or inf? \n",
    "\n",
    "# sig = np.random.randn(30 * 256)\n",
    "# sig[0] = np.nan\n",
    "# print(sig)\n",
    "# example_edf = Edf([EdfSignal(sig, sampling_frequency=256, label=\"EEG Fpz\")])\n",
    "\n",
    "# # nope! that answers that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<EdfSignal ax 400Hz>\n",
      "{'_sampling_frequency': 400.0, '_label': b'ax              ', '_transducer_type': b'                                                                                ', '_physical_dimension': b'        ', '_physical_min': b'0       ', '_physical_max': b'7.657037', '_digital_min': b'-32768  ', '_digital_max': b'32767   ', '_prefiltering': b'                                                                                ', '_samples_per_data_record': b'400     ', '_reserved': b'                                ', '_lazy_loader': None, '_digital': memmap([ 15913,  15753,  15642, ..., -32768, -32768, -32768], dtype=int16)}\n",
      "<EdfSignal derived_data_dep 50Hz>\n",
      "{'_sampling_frequency': 50.0, '_label': b'derived_data_dep', '_transducer_type': b'                                                                                ', '_physical_dimension': b'        ', '_physical_min': b'-2.00532', '_physical_max': b'-1.95383', '_digital_min': b'-32768  ', '_digital_max': b'32767   ', '_prefiltering': b'                                                                                ', '_samples_per_data_record': b'50      ', '_reserved': b'                                ', '_lazy_loader': <edfio._lazy_loading.LazyLoader object at 0xffff519a4530>}\n"
     ]
    }
   ],
   "source": [
    "for sig in divedb_edf_roundtrip.signals:\n",
    "    print(sig)\n",
    "    print(sig.__dict__)\n",
    "\n",
    "sig = divedb_edf_roundtrip.signals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_sampling_frequency': 400.0,\n",
       " '_label': b'ax              ',\n",
       " '_transducer_type': b'                                                                                ',\n",
       " '_physical_dimension': b'        ',\n",
       " '_physical_min': b'0       ',\n",
       " '_physical_max': b'7.657037',\n",
       " '_digital_min': b'-32768  ',\n",
       " '_digital_max': b'32767   ',\n",
       " '_prefiltering': b'                                                                                ',\n",
       " '_samples_per_data_record': b'400     ',\n",
       " '_reserved': b'                                ',\n",
       " '_lazy_loader': None,\n",
       " '_digital': memmap([ 15913,  15753,  15642, ..., -32768, -32768, -32768], dtype=int16)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_version': b'0       ',\n",
       " '_local_patient_identification': b'X X X X                                                                         ',\n",
       " '_local_recording_identification': b'Startdate X X X X                                                               ',\n",
       " '_startdate': b'01.01.85',\n",
       " '_starttime': b'00.00.00',\n",
       " '_bytes_in_header_record': b'768     ',\n",
       " '_reserved': b'                                            ',\n",
       " '_num_data_records': b'20      ',\n",
       " '_data_record_duration': b'1       ',\n",
       " '_num_signals': b'2   ',\n",
       " '_signals': (<EdfSignal ax 400Hz>, <EdfSignal derived_data_dep 50Hz>)}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's practice setting the other fields for the recording \n",
    "from edfio import Patient, Recording\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "edf = divedb_edf_roundtrip.copy()\n",
    "\n",
    "# Okay, looks like these additional fields are VERY strict, disallow spaces, basically can't \n",
    "# be json. According to PA, canonical thing to do here is use annotations, so we'll do that \n",
    "# for anything interesting. Single world fields/responses? allowed, in the kwarg form \n",
    "additional = ('kwarg1', 'value1', 'kwarg2', 'value2')\n",
    "\n",
    "edf.recording = Recording(\n",
    "    # startdate=datetime.date(2002, 2, 2), #TODO\n",
    "    equipment_code=\"X\", #TODO\n",
    ")\n",
    "\n",
    "path = \".tmp/prototype.edf\"\n",
    "edf.write(path)\n",
    "edf.__dict__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 X\n",
      "2 X\n",
      "3 X\n",
      "4 X\n",
      "5 blob\n",
      "6 foo\n",
      "7 X\n",
      "8 X\n",
      "9 X\n",
      "10 X\n",
      "11 X\n",
      "12 X\n",
      "13 X\n",
      "14 X\n",
      "15 X\n",
      "16 X\n",
      "17 X\n",
      "18 X\n",
      "19 X\n",
      "20 X\n",
      "21 X\n",
      "22 X\n",
      "23 X\n",
      "24 X\n",
      "25 X\n",
      "26 X\n",
      "27 X\n",
      "28 X\n",
      "29 X\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 30):\n",
    "    x = edf.recording.get_subfield(i)\n",
    "    print(i, x)\n",
    "    if x == 'X':\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_local_recording_identification': 'Startdate X X X X'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divedb_edf.recording.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huzzah! Time to clean up :) \n",
    "# ...actually false. Time to figure out how to get the metadata into the EDF header!\n",
    "# Check the edfio API: https://github.com/the-siesta-group/edfio?tab=readme-ov-file#usage \n",
    "# and https://edfio.readthedocs.io/en/stable/examples.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
