{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export EDF\n",
    "\n",
    "This notebook demonstrates the process of exporting DiveDB data as an EDF file.\n",
    "\n",
    "While under development, it also contains the prototype (non-library) code; that'll be deleted when this notebook is ready to be merged into the main branch.\n",
    "\n",
    "Punch list:\n",
    "- [x] Make a list\n",
    "- [x] Understand task :) \n",
    "- [ ] Prototype:\n",
    "    - [x] Load basic metadata\n",
    "    - [x] Load signals\n",
    "    - [x] Generate EDF file \n",
    "        - [X] Can mne serve our needs here? Check if multiple sample rates, arbitrary metadata: edfio can!\n",
    "        - [x] Decide if different library OR extend mne: use edfio, which is what mne depends on \n",
    "    - [x] Test EDF file can be opened externally (e.g. through EDF.jl or other app)\n",
    "    - [x] Test EDF encodes max/min values\n",
    "    - [ ] Add metadata to EDF header\n",
    "- [ ] In tests, write (failing) test for basic new functionality\n",
    "- [ ] Turn prototype into library code - test passes!\n",
    "- [ ] Write up edge case tests\n",
    "    - [ ] Make 'em pass OR file 'em\n",
    "- [ ] Clean up this notebook (delete this punch list!)\n",
    "- [ ] Mark PR ready for review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder: this is the end goal\n",
    "\n",
    "```python\n",
    "# Example of usage once complete\n",
    "\n",
    "from DiveDB.services.duck_pond import DuckPond\n",
    "\n",
    "duckpond = DuckPond(os.environ[\"CONTAINER_DELTA_LAKE_PATH\"])\n",
    "\n",
    "dive_data = duckpond.get_delta_data(    \n",
    "    labels=[\"eeg\"],\n",
    "    animal_ids=\"apfo-001a\",\n",
    ")\n",
    "\n",
    "dive_data.export_to_edf(\"path_to_output.edf\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(┌───────────────────────────┬───────────┬─────────────────────┐\n",
       " │         datetime          │  animal   │ derived_data_depth  │\n",
       " │ timestamp with time zone  │  varchar  │       double        │\n",
       " ├───────────────────────────┼───────────┼─────────────────────┤\n",
       " │ 2019-11-07 19:50:45+00    │ apfo-001a │ -2.0053139536656666 │\n",
       " │ 2019-11-07 19:50:45.02+00 │ apfo-001a │ -2.0053139536656666 │\n",
       " │ 2019-11-07 19:50:45.04+00 │ apfo-001a │ -2.0053139536656666 │\n",
       " │ 2019-11-07 19:50:45.06+00 │ apfo-001a │ -2.0053139536656666 │\n",
       " │ 2019-11-07 19:50:45.08+00 │ apfo-001a │ -2.0053139536656666 │\n",
       " │ 2019-11-07 19:50:45.1+00  │ apfo-001a │ -2.0053139536656666 │\n",
       " │ 2019-11-07 19:50:45.12+00 │ apfo-001a │ -2.0053139536656666 │\n",
       " │ 2019-11-07 19:50:45.14+00 │ apfo-001a │ -2.0053139536656666 │\n",
       " │ 2019-11-07 19:50:45.16+00 │ apfo-001a │ -2.0053139536656666 │\n",
       " │ 2019-11-07 19:50:45.18+00 │ apfo-001a │ -2.0053139536656666 │\n",
       " │            ·              │     ·     │           ·         │\n",
       " │            ·              │     ·     │           ·         │\n",
       " │            ·              │     ·     │           ·         │\n",
       " │ 2019-11-07 19:54:04.8+00  │ apfo-001a │  -1.157124396037812 │\n",
       " │ 2019-11-07 19:54:04.82+00 │ apfo-001a │  -1.157124396037812 │\n",
       " │ 2019-11-07 19:54:04.84+00 │ apfo-001a │  -1.157124396037812 │\n",
       " │ 2019-11-07 19:54:04.86+00 │ apfo-001a │  -1.157124396037812 │\n",
       " │ 2019-11-07 19:54:04.88+00 │ apfo-001a │  -1.157124396037812 │\n",
       " │ 2019-11-07 19:54:04.9+00  │ apfo-001a │  -1.157124396037812 │\n",
       " │ 2019-11-07 19:54:04.92+00 │ apfo-001a │  -1.157124396037812 │\n",
       " │ 2019-11-07 19:54:04.94+00 │ apfo-001a │  -1.157124396037812 │\n",
       " │ 2019-11-07 19:54:04.96+00 │ apfo-001a │  -1.157124396037812 │\n",
       " │ 2019-11-07 19:54:04.98+00 │ apfo-001a │  -1.157124396037812 │\n",
       " ├───────────────────────────┴───────────┴─────────────────────┤\n",
       " │ ? rows (>9999 rows, 20 shown)                     3 columns │\n",
       " └─────────────────────────────────────────────────────────────┘,\n",
       " <DiveDB.services.dive_data.DiveData at 0xffff6b9279e0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┌──────────┬─────────┬────────────────────────────┬─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┬──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┬───────────┐\n",
       "│ database │ schema  │            name            │                                                                                                                                                          column_names                                                                                                                                                           │                                                                                                           column_types                                                                                                           │ temporary │\n",
       "│ varchar  │ varchar │          varchar           │                                                                                                                                                            varchar[]                                                                                                                                                            │                                                                                                            varchar[]                                                                                                             │  boolean  │\n",
       "├──────────┼─────────┼────────────────────────────┼─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼───────────┤\n",
       "│ memory   │ main    │ DataLake                   │ [animal, deployment, recording, group, class, label, datetime, value]                                                                                                                                                                                                                                                           │ [VARCHAR, VARCHAR, VARCHAR, VARCHAR, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, STRUCT(\"float\" DOUBLE, string VARCHAR, \"boolean\" BOOLEAN, \"int\" BIGINT)]                                                                        │ false     │\n",
       "│ metadata │ public  │ Animal_Deployments         │ [id, animal_id, deployment_id]                                                                                                                                                                                                                                                                                                  │ [BIGINT, VARCHAR, VARCHAR]                                                                                                                                                                                                       │ false     │\n",
       "│ metadata │ public  │ Animals                    │ [id, project_id, common_name, scientific_name, birth_year, domain_ids, lab_id, sex]                                                                                                                                                                                                                                             │ [VARCHAR, VARCHAR, VARCHAR, VARCHAR, INTEGER, VARCHAR, VARCHAR, VARCHAR]                                                                                                                                                         │ false     │\n",
       "│ metadata │ public  │ Deployments                │ [id, rec_date, animal, start_time, start_time_precision, timezone, notes, deployment_name, animal_age, animal_age_class, arrival_datetime, departure_datetime, deployment_latitude, deployment_location, deployment_longitude, deployment_type, domain_deployment_id, recovery_latitude, recovery_location, recovery_longitude] │ [VARCHAR, DATE, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR, VARCHAR, INTEGER, VARCHAR, TIMESTAMP WITH TIME ZONE, TIMESTAMP WITH TIME ZONE, DOUBLE, VARCHAR, DOUBLE, VARCHAR, VARCHAR, DOUBLE, VARCHAR, DOUBLE] │ false     │\n",
       "│ metadata │ public  │ Files                      │ [id, extension, type, delta_path, recording_id, metadata, start_time, uploaded_at, file]                                                                                                                                                                                                                                        │ [BIGINT, VARCHAR, VARCHAR, VARCHAR, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, TIMESTAMP WITH TIME ZONE, VARCHAR]                                                                                                               │ false     │\n",
       "│ metadata │ public  │ Logger_Wikis               │ [id, description, tags, projects]                                                                                                                                                                                                                                                                                               │ [BIGINT, VARCHAR, VARCHAR[], VARCHAR[]]                                                                                                                                                                                          │ false     │\n",
       "│ metadata │ public  │ Loggers                    │ [id, icon_url, serial_no, manufacturer, type, type_name, notes, owner, wiki_id, manufacturer_name, ptt]                                                                                                                                                                                                                         │ [VARCHAR, VARCHAR, VARCHAR, VARCHAR, VARCHAR, VARCHAR, VARCHAR, VARCHAR, BIGINT, VARCHAR, VARCHAR]                                                                                                                               │ false     │\n",
       "│ metadata │ public  │ Media_Updates              │ [id, update_type, update_factor, file_id]                                                                                                                                                                                                                                                                                       │ [BIGINT, VARCHAR, DOUBLE, BIGINT]                                                                                                                                                                                                │ false     │\n",
       "│ metadata │ public  │ Recordings                 │ [id, start_time, actual_start_time, end_time, start_time_precision, animal_deployment_id, logger_id, name, attachment_location, attachment_type, quality, timezone]                                                                                                                                                             │ [VARCHAR, TIMESTAMP WITH TIME ZONE, TIMESTAMP WITH TIME ZONE, TIMESTAMP WITH TIME ZONE, VARCHAR, BIGINT, VARCHAR, VARCHAR, VARCHAR, VARCHAR, VARCHAR, VARCHAR]                                                                   │ false     │\n",
       "│ metadata │ public  │ auth_group                 │ [id, name]                                                                                                                                                                                                                                                                                                                      │ [INTEGER, VARCHAR]                                                                                                                                                                                                               │ false     │\n",
       "│ metadata │ public  │ auth_group_permissions     │ [id, group_id, permission_id]                                                                                                                                                                                                                                                                                                   │ [BIGINT, INTEGER, INTEGER]                                                                                                                                                                                                       │ false     │\n",
       "│ metadata │ public  │ auth_permission            │ [id, name, content_type_id, codename]                                                                                                                                                                                                                                                                                           │ [INTEGER, VARCHAR, INTEGER, VARCHAR]                                                                                                                                                                                             │ false     │\n",
       "│ metadata │ public  │ auth_user                  │ [id, password, last_login, is_superuser, username, first_name, last_name, email, is_staff, is_active, date_joined]                                                                                                                                                                                                              │ [INTEGER, VARCHAR, TIMESTAMP WITH TIME ZONE, BOOLEAN, VARCHAR, VARCHAR, VARCHAR, VARCHAR, BOOLEAN, BOOLEAN, TIMESTAMP WITH TIME ZONE]                                                                                            │ false     │\n",
       "│ metadata │ public  │ auth_user_groups           │ [id, user_id, group_id]                                                                                                                                                                                                                                                                                                         │ [BIGINT, INTEGER, INTEGER]                                                                                                                                                                                                       │ false     │\n",
       "│ metadata │ public  │ auth_user_user_permissions │ [id, user_id, permission_id]                                                                                                                                                                                                                                                                                                    │ [BIGINT, INTEGER, INTEGER]                                                                                                                                                                                                       │ false     │\n",
       "│ metadata │ public  │ django_admin_log           │ [id, action_time, object_id, object_repr, action_flag, change_message, content_type_id, user_id]                                                                                                                                                                                                                                │ [INTEGER, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, SMALLINT, VARCHAR, INTEGER, INTEGER]                                                                                                                                       │ false     │\n",
       "│ metadata │ public  │ django_content_type        │ [id, app_label, model]                                                                                                                                                                                                                                                                                                          │ [INTEGER, VARCHAR, VARCHAR]                                                                                                                                                                                                      │ false     │\n",
       "│ metadata │ public  │ django_migrations          │ [id, app, name, applied]                                                                                                                                                                                                                                                                                                        │ [BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE]                                                                                                                                                                             │ false     │\n",
       "│ metadata │ public  │ django_session             │ [session_key, session_data, expire_date]                                                                                                                                                                                                                                                                                        │ [VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE]                                                                                                                                                                                     │ false     │\n",
       "│ temp     │ main    │ pivot_results              │ [datetime, animal, derived_data_depth_float, derived_data_depth_int, derived_data_depth_bool, derived_data_depth_str]                                                                                                                                                                                                           │ [TIMESTAMP WITH TIME ZONE, VARCHAR, DOUBLE, BIGINT, BOOLEAN, VARCHAR]                                                                                                                                                            │ true      │\n",
       "├──────────┴─────────┴────────────────────────────┴─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴───────────┤\n",
       "│ 20 rows                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                6 columns │\n",
       "└──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Get metadata\n",
    "import os\n",
    "import importlib\n",
    "import DiveDB.services.duck_pond as dp\n",
    "importlib.reload(dp)\n",
    "\n",
    "duckpond = dp.DuckPond(os.environ[\"CONTAINER_DELTA_LAKE_PATH\"])\n",
    "\n",
    "# Example from the querying_docs notebook\n",
    "data = duckpond.get_delta_data(    \n",
    "    labels=[\"derived_data_depth\"],\n",
    "    animal_ids=\"apfo-001a\",\n",
    "    frequency=1/60,  # Once a minute\n",
    ")\n",
    "display(data)\n",
    "\n",
    "# Okay, but is there a way to find out what animal_ids, etc, are available?\n",
    "# Time to go spelunking!\n",
    "duckpond.get_db_schema()\n",
    "\n",
    "# ...okay, cool. :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>derived_data_corrected_gyr</td>\n",
       "      <td>gy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>derived_data_corrected_acc</td>\n",
       "      <td>az</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logger_data_CC-35</td>\n",
       "      <td>light2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logger_data_CC-35</td>\n",
       "      <td>gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sensor_data_magnetometer</td>\n",
       "      <td>mz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>sensor_data_accelerometer</td>\n",
       "      <td>ax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>derived_data_calibrated_acc</td>\n",
       "      <td>ax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>logger_data_CC-35</td>\n",
       "      <td>mz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>sensor_data_gyroscope</td>\n",
       "      <td>gx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>derived_data_calibrated_mag</td>\n",
       "      <td>my</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          class   label\n",
       "0    derived_data_corrected_gyr      gy\n",
       "1    derived_data_corrected_acc      az\n",
       "2             logger_data_CC-35  light2\n",
       "3             logger_data_CC-35      gz\n",
       "4      sensor_data_magnetometer      mz\n",
       "..                          ...     ...\n",
       "56    sensor_data_accelerometer      ax\n",
       "57  derived_data_calibrated_acc      ax\n",
       "58            logger_data_CC-35      mz\n",
       "59        sensor_data_gyroscope      gx\n",
       "60  derived_data_calibrated_mag      my\n",
       "\n",
       "[61 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's try a sql query as well (also ripped from the querying_docs notebook)\n",
    "labels_df = duckpond.conn.sql(f\"\"\"\n",
    "    SELECT label\n",
    "    FROM (\n",
    "        SELECT DISTINCT label\n",
    "        FROM DataLake\n",
    "    )\n",
    "\"\"\").df()\n",
    "# display(labels_df)\n",
    "\n",
    "animals_df = duckpond.conn.sql(f\"\"\"\n",
    "    SELECT animal\n",
    "    FROM (\n",
    "        SELECT DISTINCT animal\n",
    "        FROM DataLake\n",
    "    )\n",
    "\"\"\").df()\n",
    "# display(animals_df)\n",
    "\n",
    "signal_df = duckpond.conn.sql(f\"\"\"\n",
    "    SELECT class, label\n",
    "    FROM (\n",
    "        SELECT DISTINCT label, class\n",
    "        FROM DataLake\n",
    "    )\n",
    "\"\"\").df()\n",
    "display(signal_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['derived_data_corrected_gyr' 'derived_data_corrected_acc'\n",
      " 'logger_data_CC-35' 'sensor_data_magnetometer'\n",
      " 'derived_data_calibrated_mag' 'derived_data_calibration_acc'\n",
      " 'sensor_data_gyroscope' 'derived_data_prh' 'derived_data_calibration_mag'\n",
      " 'derived_data_inclination_angle' 'derived_data_depth' 'sensor_data_light'\n",
      " 'sensor_data_temperature' 'derived_data_corrected_mag'\n",
      " 'sensor_data_accelerometer' 'derived_data_calibrated_acc'\n",
      " 'sensor_data_pressure']\n"
     ]
    }
   ],
   "source": [
    "signal_df.sort_values(by=\"class\")\n",
    "print(signal_df['class'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commenting out b/c otherwise this crashes my kernel (if i do other stuff after it)\n",
    "\n",
    "# # Once more from the other notebook....\n",
    "# # Get the filtered data\n",
    "# resampled_data = duckpond.get_delta_data(    \n",
    "#     animal_ids=\"apfo-001a\",\n",
    "#     # Resample values to 100 Hz and make sure each signal has the same time intervals\n",
    "#     frequency=100,\n",
    "#     # Aggregation of events (think state events - behaviors) type: state (has state and end dates)\n",
    "#     classes=\"sensor_data_accelerometer\",\n",
    "# )\n",
    "# display(resampled_data)\n",
    "# # Huh. okay, `frequency` triggering a materialization + resample is interesting, not sure \n",
    "# # I would have guessed that from the API! I would have guessed that had to do with \n",
    "# # the sampling rate of the recording.\n",
    "\n",
    "# # Okay, so the output of `get_delta_data` with a set frequency returns the signal as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f70be5bf88c4b0b94c88d4dfce591d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(┌─────────────────────────────┬───────────┬───────────────┬────────────────┬───────────────┐\n",
       " │          datetime           │  animal   │      ax       │       az       │      ay       │\n",
       " │  timestamp with time zone   │  varchar  │    double     │     double     │    double     │\n",
       " ├─────────────────────────────┼───────────┼───────────────┼────────────────┼───────────────┤\n",
       " │ 2019-11-07 19:50:45+00      │ apfo-001a │ -0.0071826049 │ -10.6901104125 │  0.0263362182 │\n",
       " │ 2019-11-07 19:50:45.0025+00 │ apfo-001a │  0.0167594116 │  -10.637437976 │  0.0407014282 │\n",
       " │ 2019-11-07 19:50:45.005+00  │ apfo-001a │ -0.0167594116 │ -10.6565915893 │  0.0430956298 │\n",
       " │ 2019-11-07 19:50:45.0075+00 │ apfo-001a │  0.0239420166 │ -10.7356002441 │  0.0430956298 │\n",
       " │ 2019-11-07 19:50:45.01+00   │ apfo-001a │  0.0023942016 │ -10.6158901611 │  0.0478840332 │\n",
       " │ 2019-11-07 19:50:45.0125+00 │ apfo-001a │  0.0023942016 │ -10.7236292358 │  0.0622492431 │\n",
       " │ 2019-11-07 19:50:45.015+00  │ apfo-001a │  0.0526724365 │ -10.6637741943 │  0.0287304199 │\n",
       " │ 2019-11-07 19:50:45.0175+00 │ apfo-001a │  0.0071826049 │ -10.6565915893 │  0.0454898315 │\n",
       " │ 2019-11-07 19:50:45.02+00   │ apfo-001a │  0.0119710083 │  -10.644620581 │  0.0454898315 │\n",
       " │ 2019-11-07 19:50:45.0225+00 │ apfo-001a │ -0.0047884033 │ -10.6733510009 │  0.0885854614 │\n",
       " │             ·               │     ·     │        ·      │        ·       │        ·      │\n",
       " │             ·               │     ·     │        ·      │        ·       │        ·      │\n",
       " │             ·               │     ·     │        ·      │        ·       │        ·      │\n",
       " │ 2019-11-07 19:51:09.975+00  │ apfo-001a │  0.9888052856 │  -10.292672937 │ -0.6177040283 │\n",
       " │ 2019-11-07 19:51:09.9775+00 │ apfo-001a │  0.9768342773 │ -10.2040874755 │ -0.6464344482 │\n",
       " │ 2019-11-07 19:51:09.98+00   │ apfo-001a │  0.9217676391 │ -10.2232410888 │ -0.6368576416 │\n",
       " │ 2019-11-07 19:51:09.9825+00 │ apfo-001a │  0.9672574707 │ -10.2950671386 │ -0.6344634399 │\n",
       " │ 2019-11-07 19:51:09.985+00  │ apfo-001a │  0.9457096557 │  -10.165780249 │ -0.5961562133 │\n",
       " │ 2019-11-07 19:51:09.9875+00 │ apfo-001a │  1.0414777221 │  -10.151415039 │ -0.5291185668 │\n",
       " │ 2019-11-07 19:51:09.99+00   │ apfo-001a │  0.9816226806 │  -10.151415039 │ -0.4596867187 │\n",
       " │ 2019-11-07 19:51:09.9925+00 │ apfo-001a │  0.9864110839 │ -10.2447889038 │ -0.4836287353 │\n",
       " │ 2019-11-07 19:51:09.995+00  │ apfo-001a │  1.0295067138 │ -10.2304236938 │ -0.4405331054 │\n",
       " │ 2019-11-07 19:51:09.9975+00 │ apfo-001a │   1.077390747 │ -10.1346556274 │ -0.4548983154 │\n",
       " ├─────────────────────────────┴───────────┴───────────────┴────────────────┴───────────────┤\n",
       " │ ? rows (>9999 rows, 20 shown)                                                  5 columns │\n",
       " └──────────────────────────────────────────────────────────────────────────────────────────┘,\n",
       " <DiveDB.services.dive_data.DiveData at 0xffff80319970>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Is there a way to get the original sample rate? \n",
    "unmaterialized_data = duckpond.get_delta_data(    \n",
    "    animal_ids=\"apfo-001a\",\n",
    "    # Resample values to 10 Hz and make sure each signal has the same time intervals\n",
    "    frequency=None,\n",
    "    # Aggregation of events (think state events - behaviors) type: state (has state and end dates)\n",
    "    classes=\"sensor_data_accelerometer\",\n",
    ")\n",
    "display(unmaterialized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "# ... okay, got it. now, let's do what needs doing. \n",
    "# But also, keep in mind that we should NOT pass a frequency into `get_delta_data`\n",
    "# before EDF export unless we are very explicit about what we are doing and why. \n",
    "\n",
    "# When we don't pass in a frequency (i.e., resample), we get a DuckDBPyRelation\n",
    "# out of `get_delta_data`\n",
    "print(type(unmaterialized_data))\n",
    "\n",
    "# ...from task, I think we want a DuckDBPyConnection instead? Currently unclear to me\n",
    "# how these interop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<EdfSignal EEG Fpz 256Hz>, <EdfSignal Body Temp 1Hz>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.77000165, -0.20818987,  0.10912007, ...,  1.57440918,\n",
       "        0.61083287, -0.44157653])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Okay, now to an EDF! \n",
    "# Let's do the demo from edfio (what mne depends on for its EDF support)\n",
    "\n",
    "from edfio import Edf, EdfSignal, read_edf\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# edfio's example\n",
    "example_edf = Edf(\n",
    "    [\n",
    "        EdfSignal(np.random.randn(30 * 256), sampling_frequency=256, label=\"EEG Fpz\"),\n",
    "        EdfSignal(np.random.randn(30), sampling_frequency=1, label=\"Body Temp\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "outpath = \".tmp/example.edf\"\n",
    "example_edf.write(outpath)\n",
    "\n",
    "example_edf_roundtrip = read_edf(outpath)\n",
    "display(example_edf_roundtrip.signals)\n",
    "display(example_edf_roundtrip.signals[0].data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aedf533fe644d5cb0b7b35f6926d278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'df'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m max_duration_sec \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m class_name \u001b[38;5;129;01min\u001b[39;00m classes:\n\u001b[1;32m      9\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mduckpond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_delta_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43manimal_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapfo-001a\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m---> 11\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m()\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m#TODO-optimize: surely there is a way to get the number of samples without loading them all?? \u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# If so, do that!\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     sampling_rate \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdiff()[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mtotal_seconds()\u001b[38;5;241m.\u001b[39munique()[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'df'"
     ]
    }
   ],
   "source": [
    "# ...and now with our data!\n",
    "# Can we make an EDF from our data? \n",
    "# intentionally picking signals with different sampling rates\n",
    "classes = [\"sensor_data_accelerometer\",\"sensor_data_pressure\", \"derived_data_prh\"]\n",
    "\n",
    "# Set up for EDF - first figure out what common max duration is, etc\n",
    "max_duration_sec = 0\n",
    "for class_name in classes:\n",
    "    df = duckpond.get_delta_data(animal_ids=\"apfo-001a\",\n",
    "                                 classes=[class_name],\n",
    "                                 ).df()\n",
    "\n",
    "    #TODO-optimize: surely there is a way to get the number of samples without loading them all?? \n",
    "    # If so, do that!\n",
    "    sampling_rate = df[\"datetime\"].diff()[1:].dt.total_seconds().unique()[0]\n",
    "    sampling_frequency = int(1/sampling_rate) # TODO-safety: don't just blindly round o_O\n",
    "\n",
    "    # TODO-correctly! need to figure out max signal length, then start time, then \n",
    "    # Lpad to the correct start time + lpad to the correct stop time (lol EDF)\n",
    "    # For now, we're faking it. We happen to know that the maximum duration signal \n",
    "    # of these two is 20 s, so lets zero-pad to that:\n",
    "    sig_max_duration_sec = math.ceil(df.shape[0] / sampling_frequency)\n",
    "    max_duration_sec = max(max_duration_sec, sig_max_duration_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now actually use the real data to create an edf\n",
    "signals = []\n",
    "for class_name in classes:\n",
    "    df = duckpond.get_delta_data(    \n",
    "        animal_ids=\"apfo-001a\",\n",
    "        classes=[class_name],\n",
    "    ).df()\n",
    "\n",
    "    # TODO-safety: check that there's only one value after the unique, check that \n",
    "    # this is an integer value or whatever the EDF spec requires, etc\n",
    "    sampling_rate = df[\"datetime\"].diff()[1:].dt.total_seconds().unique()[0]\n",
    "    sampling_frequency = int(1/sampling_rate) # TODO-safety: don't just blindly round, see if we allow floating point values here also?? o_O\n",
    "\n",
    "    if class_name.startswith(\"sensor_data\"):\n",
    "        class_prefix = class_name[12:]\n",
    "    elif class_name.startswith(\"derived_data\"):\n",
    "        class_prefix = \"**\" + class_name[13:]\n",
    "    else:\n",
    "        class_prefix = class_name\n",
    "\n",
    "    # Need to figure out max signal length, then start time, then \n",
    "    # Lpad to the correct start time + lpad to the correct stop time (lol EDF)\n",
    "    # TODO-future: instead of padding w/ 0, use some signal-specific value\n",
    "    num_channels = df.shape[1] - 1\n",
    "    i_sample_start_offset = 0 #TODO - make sure this is set to the signal's offset\n",
    "    for i_channel in range(0, num_channels):\n",
    "        signal_data = np.zeros(max_duration_sec * sampling_frequency, dtype=np.float64)\n",
    "        channel_label = (df.columns)[i_channel + 1]\n",
    "\n",
    "        # TODO-future safety: make sure signal labels are unique across recording\n",
    "        # TODO-future: pull into own function\n",
    "        if class_name == channel_label:\n",
    "            signal_label = class_prefix if len(class_prefix) <= 16 else class_prefix[0:16]  # lol EDF\n",
    "        else:\n",
    "            max_prefix_length = 16 - len(channel_label) - 1  # lol EDF \n",
    "            # TODO handle case when prefix is now < 0\n",
    "            signal_prefix = class_prefix if len(class_prefix) <= max_prefix_length else label_prefix[0:max_prefix_length]\n",
    "            signal_label = signal_prefix + \"-\" + channel_label\n",
    "\n",
    "        num_samples = len(df[channel_label].values)\n",
    "        signal_data[i_sample_start_offset:num_samples] = df[channel_label].values\n",
    "\n",
    "        signal = EdfSignal(signal_data,\n",
    "                           sampling_frequency=sampling_frequency, \n",
    "                           label=signal_label)\n",
    "        # TODO-add header metadata\n",
    "        signals.append(signal)\n",
    "    \n",
    "divedb_edf = Edf(signals)\n",
    "path = \".tmp/prototype.edf\"\n",
    "divedb_edf.write(path)\n",
    "\n",
    "divedb_edf_roundtrip = read_edf(path)\n",
    "display(divedb_edf_roundtrip.signals)\n",
    "display(divedb_edf_roundtrip.signals[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = \"sensor_data_accelerometer\"\n",
    "df = duckpond.get_delta_data(    \n",
    "    animal_ids=\"apfo-001a\",\n",
    "    classes=[class_name],\n",
    "    # limit=1000,\n",
    ").df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commenting out to avoid an unnecessary error! Uncomment to see the (fully expected) error. :) \n",
    "# # Can an edf contain nan or inf? \n",
    "\n",
    "# sig = np.random.randn(30 * 256)\n",
    "# sig[0] = np.nan\n",
    "# print(sig)\n",
    "# example_edf = Edf([EdfSignal(sig, sampling_frequency=256, label=\"EEG Fpz\")])\n",
    "\n",
    "# # nope! that answers that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sig in divedb_edf_roundtrip.signals:\n",
    "    print(sig)\n",
    "    print(sig.__dict__)\n",
    "\n",
    "sig = divedb_edf_roundtrip.signals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's practice setting the other fields for the recording \n",
    "from edfio import Patient, Recording\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "edf = divedb_edf_roundtrip.copy()\n",
    "\n",
    "# Okay, looks like these additional fields are VERY strict, disallow spaces, basically can't \n",
    "# be json. According to PA, canonical thing to do here is use annotations, so we'll do that \n",
    "# for anything interesting. Single world fields/responses? allowed, in the kwarg form \n",
    "additional = ('kwarg1', 'value1', 'kwarg2', 'value2')\n",
    "\n",
    "edf.recording = Recording(\n",
    "    # startdate=datetime.date(2002, 2, 2), #TODO\n",
    "    equipment_code=\"X\", #TODO\n",
    ")\n",
    "\n",
    "path = \".tmp/prototype.edf\"\n",
    "edf.write(path)\n",
    "edf.__dict__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divedb_edf.recording.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huzzah! Time to clean up :) \n",
    "# ...actually false. Time to figure out how to get the metadata into the EDF header!\n",
    "# Check the edfio API: https://github.com/the-siesta-group/edfio?tab=readme-ov-file#usage \n",
    "# and https://edfio.readthedocs.io/en/stable/examples.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "061d2629967b4c2da98dd7326d880740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'DiveDB.services.dive_data.DiveData'>\n",
      "                           datetime     animal  derived_data_depth\n",
      "0         2019-11-07 19:50:45+00:00  apfo-001a           -2.005314\n",
      "1  2019-11-07 19:50:45.020000+00:00  apfo-001a           -2.005314\n",
      "2  2019-11-07 19:50:45.040000+00:00  apfo-001a           -2.005314\n",
      "3  2019-11-07 19:50:45.060000+00:00  apfo-001a           -2.005314\n",
      "4  2019-11-07 19:50:45.080000+00:00  apfo-001a           -2.005314\n",
      "..                              ...        ...                 ...\n",
      "95 2019-11-07 19:50:46.900000+00:00  apfo-001a           -1.983105\n",
      "96 2019-11-07 19:50:46.920000+00:00  apfo-001a           -1.983105\n",
      "97 2019-11-07 19:50:46.940000+00:00  apfo-001a           -1.983105\n",
      "98 2019-11-07 19:50:46.960000+00:00  apfo-001a           -1.983105\n",
      "99 2019-11-07 19:50:46.980000+00:00  apfo-001a           -1.983105\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Okay, how does this work now?\n",
    "import DiveDB.services.duck_pond as dp\n",
    "importlib.reload(dp)\n",
    "import DiveDB.services.dive_data as dd\n",
    "importlib.reload(dd)\n",
    "\n",
    "duckpond = dp.DuckPond(os.environ[\"CONTAINER_DELTA_LAKE_PATH\"])\n",
    "\n",
    "results = duckpond.get_delta_data(    \n",
    "    classes=[\"derived_data_depth\", \"sensor_data_accelerometer\"],\n",
    "    animal_ids=\"apfo-001a\",\n",
    "    limit=100, # 0000\n",
    ")\n",
    "print(type(results))\n",
    "print(results.df())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
