{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Uploader\n",
    "\n",
    "This notebook demonstrates the process of uploading EDF files data to Delta Lake and OpenStack Swift for long-term storage. \n",
    "\n",
    "It also includes the setup and execution of the data upload process, as well as querying the uploaded data for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the servers:\n",
    "To launch the server, open the Docker Desktop app and run the following command at the root of the project:\n",
    "```bash\n",
    "$ make up\n",
    "```\n",
    "This command will launch the Django server, Postgres database, and Jupyter server using the environment variables defined in the `.env` file accross all containers.\n",
    "\n",
    "#### Understanding expected file paths:\n",
    "DiveDB expects the following paths to be set in the `.env` file:\n",
    "- `CONTAINER_DATA_PATH`\n",
    "- `LOCAL_DATA_PATH`\n",
    "- `HOST_DELTA_LAKE_PATH`\n",
    "- `CONTAINER_DELTA_LAKE_PATH`\n",
    "\n",
    "These paths are used to mount the Delta Lake and file storage to the containers. The \"LOCAL_\" and \"HOST_\" paths can be wherever makes sense for your local machine. The \"CONTAINER_\" paths are the paths that the containers expect. We recommend you keep the \"CONTAINER_\" paths as they are in the `.env.example` file.\n",
    "\n",
    "#### When is the server ready?\n",
    "There are 3 processes that need to be running for the server to be ready:\n",
    "1. The Django server (`web`)\n",
    "2. The Postgres database (`postgres`)\n",
    "3. The Jupyter server (`jupyter`)\n",
    "\n",
    "Jupyter is almost always the last to start up. You'll know it's ready when you see the following logs in the terminal:\n",
    "```bash\n",
    "jupyter-1            | [I 2024-08-30 16:12:37.083 ServerApp] Serving notebooks from local directory: /app\n",
    "jupyter-1            | [I 2024-08-30 16:12:37.083 ServerApp] Jupyter Server 2.14.2 is running at:\n",
    "jupyter-1            | [I 2024-08-30 16:12:37.083 ServerApp] http://e29d05e13fd0:8888/jupyter/tree\n",
    "jupyter-1            | [I 2024-08-30 16:12:37.083 ServerApp]     http://127.0.0.1:8888/jupyter/tree\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to the Jupyter Kernel:\n",
    "To connect to the Jupyter server in your notebook, follow these steps:\n",
    "1. Click the \"Select Kernel\" button at the top right of the page.\n",
    "1. Pick the \"Select another kernel\" option in the dropdown menu.\n",
    "1. Pick the \"Existing Jupyter Server\" option in the dropdown menu.\n",
    "1. Now we need to connect to the Jupyter server.\n",
    "    - If you previously connected to the Jupyter server\n",
    "        - Pick the \"localhost\" option in the dropdown menu (or whatever you named it prior)\n",
    "    - If you have not connected to the Jupyter server before\n",
    "        - Pick the \"Enter the URL of the running Jupyter server\" option in the dropdown menu.\n",
    "        - Enter http://localhost:8888/jupyter\n",
    "        - Give it a name you'll remember (like \"Local DiveDB Jupyter Server\")\n",
    "1. Press the \"Reload\" icon in the top right of the dropdown menu to see the latest kernel.\n",
    "1. Pick the \"Python 3\" option in the dropdown menu.\n",
    "\n",
    "This will ensure you execute the Jupyter notebook in the correct environment.\n",
    "\n",
    "After connecting to the Jupyter server, ensure your notebook runs this command to set the appropriate file paths:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing to upload data:\n",
    "There are two aspects to any data upload:\n",
    "1. A netCDF file containing measurements and time\n",
    "2. The metadata for the measurements\n",
    "    - This describes the context of the measurements using the following fields:\n",
    "        - animal\n",
    "        - deployment\n",
    "        - logger\n",
    "\n",
    "There are several ways to define your metadata. \n",
    "\n",
    "#### Supplied Metadata Dictionary\n",
    "If you know the metadata for your measurements, you can pass a dictionary to the `upload_netcdf` function. The dictionary should represent metadata existing in the Metadata database and contain the following fields:\n",
    "- animal: The animal ID\n",
    "- deployment: The deployment name\n",
    "- recording: The recording name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading netCDF files\n",
    "The `netcdf_file_path` list contains the paths to the netCDF files that we want to upload. It can point to files on your local machine or on a remote server.\n",
    "In this example, the file is located in the ../data/files/ directory and is named deployment_data.nc.\n",
    "\n",
    "The upload_netcdf function will perform the following: \n",
    "- use the provided metadata dictionary to extract the metadata for your measurements\n",
    "- upload copies of the netCDF files to OpenStack Swift\n",
    "- upload the measurements to Delta Lake\n",
    "\n",
    "The process takes between 30 secs to 1 min to complete per gigabyte â€” about 2/3rds of the time is used to upload the files to OpenStack Swift. (*note: we can speed this up by parellizing the upload process*)\n",
    "\n",
    "### Example netCDF File\n",
    "An example netCDF file can be downloaded here: [https://figshare.com/ndownloader/files/50061330](https://figshare.com/ndownloader/files/50061330) that meets the above requirements and can be used as a template for your own data.\n",
    "\n",
    "### Example 1: Uploading a netCDF file when metadata is already in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "import xarray as xr\n",
    "\n",
    "os.environ[\"SKIP_OPENSTACK_UPLOAD\"] = \"true\"\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"\n",
    "\n",
    "from DiveDB.services.duck_pond import DuckPond\n",
    "\n",
    "import DiveDB.services.data_uploader\n",
    "importlib.reload(DiveDB.services.data_uploader)\n",
    "from DiveDB.services.data_uploader import DataUploader\n",
    "\n",
    "duckpond = DuckPond(os.environ[\"CONTAINER_DELTA_LAKE_PATH\"])\n",
    "data_uploader = DataUploader()\n",
    "\n",
    "# The example netCDF file can be downloaded here: [https://figshare.com/ndownloader/files/50061330](https://figshare.com/ndownloader/files/50061330)\n",
    "example_data_path = \"./files/example_data.nc\"\n",
    "\n",
    "# Prepare data for each model\n",
    "with xr.open_dataset(example_data_path) as ds:\n",
    "    animal_id = ds.attrs.get(\"animal_info_page_id\")\n",
    "    deployment_id = ds.attrs.get(\"deployment_info_page_id\")\n",
    "    \n",
    "    sensor_info_attrs = {key: value for key, value in ds.attrs.items() if key.startswith(\"sensor_info\")}\n",
    "    sensor_info_words = list(set(key.split(\"sensor_info_\")[1].split(\"_\")[0] for key in sensor_info_attrs))\n",
    "    logger_ids = {ds.attrs.get(f\"sensor_info_{word}_logger_id\") for word in sensor_info_words}\n",
    "    \n",
    "    if len(logger_ids) == 1:\n",
    "        logger_id = list(logger_ids)[0]\n",
    "        metadata = {\n",
    "            \"animal\": animal_id,\n",
    "            \"deployment\": deployment_id,\n",
    "            \"recording\": f\"{deployment_id}_{animal_id}_{logger_id}\"\n",
    "        }\n",
    "\n",
    "        data_uploader.upload_netcdf(example_data_path, metadata)\n",
    "    else:\n",
    "        print(\"Multiple loggers detected. Divide data into separate files for each logger.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Uploading a netCDF file when metadata is not in the database\n",
    "\n",
    "If the metadata is not in the database, the upload_netcdf function require you to provide the data manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "import xarray as xr\n",
    "\n",
    "os.environ[\"SKIP_OPENSTACK_UPLOAD\"] = \"true\"\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"\n",
    "\n",
    "from DiveDB.services.duck_pond import DuckPond\n",
    "\n",
    "import DiveDB.services.data_uploader\n",
    "importlib.reload(DiveDB.services.data_uploader)\n",
    "from DiveDB.services.data_uploader import DataUploader\n",
    "\n",
    "duckpond = DuckPond(os.environ[\"CONTAINER_DELTA_LAKE_PATH\"])\n",
    "data_uploader = DataUploader()\n",
    "\n",
    "# The example netCDF file can be downloaded here: [https://figshare.com/ndownloader/files/50061330](https://figshare.com/ndownloader/files/50061330)\n",
    "example_data_path = \"./files/example.nc\"\n",
    "\n",
    "# Prepare data for each model\n",
    "with xr.open_dataset(example_data_path) as ds:\n",
    "    display(ds)\n",
    "    animal_data = {\n",
    "        \"animal_id\": ds.attrs.get(\"animal_info_page_id\"),\n",
    "        \"project_id\": ds.attrs.get(\"animal_info_Project ID\"),\n",
    "        \"scientific_name\": ds.attrs.get(\"animal_info_Scientific Name\"),\n",
    "        \"common_name\": ds.attrs.get(\"animal_info_Common Name\"),\n",
    "        \"lab_id\": ds.attrs.get(\"animal_info_Lab ID\"),\n",
    "        \"birth_year\": -999,  # Animal age is not provided in the example data\n",
    "        \"sex\": ds.attrs.get(\"animal_info_Sex\"),\n",
    "        \"domain_ids\": ds.attrs.get(\"animal_info_Domain IDs\"),\n",
    "    }\n",
    "\n",
    "    deployment_data = {\n",
    "        \"deployment_id\": ds.attrs.get(\"deployment_info_page_id\"),\n",
    "        \"domain_deployment_id\": ds.attrs.get(\"deployment_info_Domain Deployment ID\"),\n",
    "        \"animal_age_class\": ds.attrs.get(\"deployment_info_Animal Age Class\"),\n",
    "        \"animal_age\": -999,  # Animal age is not provided in the example data\n",
    "        \"deployment_type\": ds.attrs.get(\"deployment_info_Deployment Type\"),\n",
    "        \"deployment_name\": ds.attrs.get(\"deployment_info_page_id\"),\n",
    "        \"rec_date\": ds.attrs.get(\"deployment_info_Recording Date\"),\n",
    "        \"deployment_latitude\": ds.attrs.get(\"deployment_info_Deployment Latitude\"),\n",
    "        \"deployment_longitude\": ds.attrs.get(\"deployment_info_Deployment Longitude\"),\n",
    "        \"deployment_location\": ds.attrs.get(\"deployment_info_Deployment Location\"),\n",
    "        \"departure_datetime\": ds.attrs.get(\"deployment_info_Departure Datetime\"),\n",
    "        \"timezone\": ds.attrs.get(\"deployment_info_Time Zone\"),\n",
    "        \"recovery_latitude\": ds.attrs.get(\"deployment_info_Recovery Latitude\"),\n",
    "        \"recovery_longitude\": ds.attrs.get(\"deployment_info_Recovery Longitude\"),\n",
    "        \"recovery_location\": ds.attrs.get(\"deployment_info_Recovery Location\"),\n",
    "        \"arrival_datetime\": ds.attrs.get(f\"deployment_info_Recording Date\") + \" \" + ds.attrs.get(f\"deployment_info_Start Time\"),\n",
    "        \"notes\": ds.attrs.get(\"deployment_info_Notes\"),\n",
    "    }\n",
    "\n",
    "    # Create or get records\n",
    "    animal, _ = data_uploader.get_or_create_animal(animal_data)\n",
    "    deployment, _ = data_uploader.get_or_create_deployment(deployment_data)\n",
    "    \n",
    "    sensor_info_attrs = {key: value for key, value in ds.attrs.items() if key.startswith(\"sensor_info\")}\n",
    "    sensor_info_words = list(set(key.split(\"sensor_info_\")[1].split(\"_\")[0] for key in sensor_info_attrs))\n",
    "    logger_ids = {ds.attrs.get(f\"sensor_info_{word}_logger_id\") for word in sensor_info_words}\n",
    "    \n",
    "    if len(logger_ids) == 1:\n",
    "        logger_label = sensor_info_words[0]\n",
    "        logger_data = {\n",
    "            \"logger_id\": ds.attrs.get(f\"sensor_info_{logger_label}_logger_id\"),\n",
    "            \"manufacturer\": ds.attrs.get(f\"sensor_info_{logger_label}_logger_manufacturer\"),\n",
    "            \"manufacturer_name\": ds.attrs.get(f\"sensor_info_{logger_label}_logger_model\"),\n",
    "            \"serial_no\": ds.attrs.get(f\"sensor_info_{logger_label}_logger_serial_number\"),\n",
    "            \"ptt\": ds.attrs.get(f\"sensor_info_{logger_label}_logger_ptt\"),\n",
    "            \"type\": ds.attrs.get(f\"sensor_info_{logger_label}_logger_type\"),\n",
    "            \"notes\": ds.attrs.get(f\"sensor_info_{logger_label}_details\"),\n",
    "        }\n",
    "        \n",
    "        logger, _ = data_uploader.get_or_create_logger(logger_data)\n",
    "        \n",
    "        recording_data = {\n",
    "            \"recording_id\": f\"{deployment_data[\"deployment_id\"]}_{animal_data[\"animal_id\"]}_{logger_data[\"logger_id\"]}\",\n",
    "            \"name\": f\"{deployment_data[\"deployment_id\"]}_{animal_data[\"animal_id\"]}_{logger_data[\"logger_id\"]}\",\n",
    "            \"animal\": animal,\n",
    "            \"deployment\": deployment,\n",
    "            \"logger\": logger,\n",
    "            \"start_time\": ds.attrs.get(f\"sensor_info_{logger_label}_sensor_start_datetime\"),\n",
    "            \"end_time\": ds.attrs.get(f\"sensor_info_{logger_label}_sensor_end_datetime\"),\n",
    "            \"timezone\": ds.attrs.get(\"Time_Zone\"),\n",
    "            \"quality\": ds.attrs.get(\"Quality\"),\n",
    "            \"attachment_location\": ds.attrs.get(\"Attachment_Location\"),\n",
    "            \"attachment_type\": ds.attrs.get(\"Attachment_Type\"),\n",
    "        }\n",
    "\n",
    "        recording, _ = data_uploader.get_or_create_recording(recording_data)\n",
    "\n",
    "        metadata = {\n",
    "            \"animal\": animal.id,\n",
    "            \"deployment\": deployment.id,\n",
    "            \"recording\": recording.id,\n",
    "        } \n",
    "\n",
    "        data_uploader.upload_netcdf(example_data_path, metadata)\n",
    "\n",
    "    else:\n",
    "        print(\"Multiple loggers detected. Divide data into separate files for each logger.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
