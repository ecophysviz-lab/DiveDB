{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Uploader\n",
    "\n",
    "This notebook demonstrates the process of uploading Pyologger-prepared data in NetCDFs to DiveDB's Iceberg storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting your environment (Docker or Local)\n",
    "DiveDB can be run either within a Docker container or directly on your local machine. Both approaches rely on the same .env configuration file for environment variables, but differ in how file paths are handled. Docker will more closely match a cloud-hosted environment, but local runs are quicker and easier to set up.\n",
    "\n",
    "###Option 1: Run Jupyter server with Docker\n",
    "\n",
    "To launch the server using Docker, open the Docker Desktop app and run the following command at the root of the project:\n",
    "```bash\n",
    "$ make up\n",
    "```\n",
    "This command will launch the Jupyter server using the environment variables defined in your .env file.\n",
    "\n",
    "#### Understanding expected file paths (Docker):\n",
    "\n",
    "For local uploads in Docker, DiveDB expects the following variables to be set in your .env file:\n",
    "- `LOCAL_DATA_PATH`: The path to the local data directory (source of data to be uploaded).\n",
    "- `LOCAL_ICEBERG_PATH`: The path to the local iceberg directory (destination of data to be uploaded).\n",
    "- `CONTAINER_DATA_PATH`: The path that the container will see for the data directory.\n",
    "- `CONTAINER_ICEBERG_PATH`: The path that the container will see for the iceberg directory.\n",
    "\n",
    "The “LOCAL_” paths refer to folders on your machine. The “CONTAINER_” paths define how those folders are mounted inside the container. We recommend keeping the “CONTAINER_” paths consistent with the values provided in .env.example.\n",
    "\n",
    "#### Uploads to S3 (Docker):\n",
    "\n",
    "If uploading to S3, the following variables must also be defined:\n",
    "- `S3_ENDPOINT`: The endpoint of the S3 bucket.\n",
    "- `S3_ACCESS_KEY`: The access key of the S3 bucket.\n",
    "- `S3_SECRET_KEY`: The secret key of the S3 bucket.\n",
    "- `S3_BUCKET`: Just the name of the S3 bucket.\n",
    "\n",
    "These values are used regardless of whether you run DiveDB in Docker or locally.\n",
    "\n",
    "When is the server ready?\n",
    "\n",
    "You’ll know the server is ready when you see logs like the following:\n",
    "```\n",
    "jupyter-1            | [I 2024-08-30 16:12:37.083 ServerApp] Serving notebooks from local directory: /app\n",
    "jupyter-1            | [I 2024-08-30 16:12:37.083 ServerApp] Jupyter Server 2.14.2 is running at:\n",
    "jupyter-1            | [I 2024-08-30 16:12:37.083 ServerApp] http://e29d05e13fd0:8888/jupyter/tree\n",
    "jupyter-1            | [I 2024-08-30 16:12:37.083 ServerApp]     http://127.0.0.1:8888/jupyter/tree\n",
    "```\n",
    "\n",
    "#### Connecting to the Jupyter Kernel from within VSCode\n",
    "\n",
    "The following steps assume that you're working within VSCode, with both the Python and Jupyter extensions installed. For other workflows, you'll need to follow your workflow-specific steps to connect to the Jupyter Kernel launched by `make up`.\n",
    "\n",
    "To connect to the Jupyter server in your notebook, follow these steps:\n",
    "1. Click the \"Select Kernel\" button at the top right of the page.\n",
    "1. Pick the \"Select another kernel\" option in the dropdown menu.\n",
    "1. Pick the \"Existing Jupyter Server\" option in the dropdown menu.\n",
    "1. Now we need to connect to the Jupyter server.\n",
    "    - If you previously connected to the Jupyter server\n",
    "        - Pick the \"localhost\" option in the dropdown menu (or whatever you named it prior)\n",
    "    - If you have not connected to the Jupyter server before\n",
    "        - Pick the \"Enter the URL of the running Jupyter server\" option in the dropdown menu.\n",
    "        - Enter http://localhost:8888/jupyter\n",
    "        - Give it a name you'll remember (like \"Local DiveDB Jupyter Server\")\n",
    "1. Press the \"Reload\" icon in the top right of the dropdown menu to see the latest kernel.\n",
    "1. Pick the \"Python 3\" option in the dropdown menu.\n",
    "\n",
    "This will ensure you execute the Jupyter notebook in the correct environment.\n",
    "\n",
    "### Option 2: Run Jupyter server locally (no Docker)\n",
    "\n",
    "You can also run the server directly on your machine, without Docker.\n",
    "\n",
    "The environment variables from .env will still be used—consider loading them into your shell manually (e.g., using dotenv or exporting them yourself), or structure your notebook to load them directly with python-dotenv.\n",
    "\n",
    "#### Understanding expected file paths (Local):\n",
    "\n",
    "If running locally (outside Docker), the following variables are still expected:\n",
    "- `LOCAL_DATA_PATH`: The path to the local data directory.\n",
    "- `LOCAL_ICEBERG_PATH`: The path to the local iceberg directory.\n",
    "\n",
    "Unlike the Docker setup, the container-specific variables (CONTAINER_DATA_PATH, CONTAINER_ICEBERG_PATH) are not used. Any paths referenced in your code should point directly to the LOCAL_ paths.\n",
    "\n",
    "Uploads to S3 (Local):\n",
    "\n",
    "S3 configuration remains unchanged. The same variables must be set:\n",
    "- `S3_ENDPOINT`: The endpoint of the S3 bucket.\n",
    "- `S3_ACCESS_KEY`: The access key of the S3 bucket.\n",
    "- `S3_SECRET_KEY`: The secret key of the S3 bucket.\n",
    "- `S3_BUCKET`: The name of the S3 bucket.\n",
    "\n",
    "⸻\n",
    "\n",
    "Choose the option that best fits your need. Docker provides a consistent, containerized environment, while the local setup is more flexible for quick iteration and debugging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing to upload data:\n",
    "There are two aspects to any data upload:\n",
    "1. A netCDF file containing measurements and time\n",
    "2. The metadata for the measurements\n",
    "    - This describes the context of the measurements using the following fields:\n",
    "        - dataset\n",
    "        - animal\n",
    "        - deployment\n",
    "        - logger\n",
    "\n",
    "There are several ways to define your metadata. \n",
    "\n",
    "#### Supplied Metadata Dictionary\n",
    "If you know the metadata for your measurements, you can pass a dictionary to the `upload_netcdf` function. The dictionary should represent metadata existing in the Metadata database and contain the following fields:\n",
    "- animal: The animal ID\n",
    "- deployment: The deployment name\n",
    "- recording: The recording name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading netCDF files\n",
    "The `netcdf_file_path` list contains the paths to the netCDF files that we want to upload. It can point to files on your local machine or on a remote server.\n",
    "In this example, the file is located in the ../data/files/ directory and is named deployment_data.nc.\n",
    "\n",
    "The upload_netcdf function will perform the following: \n",
    "- use the provided metadata dictionary to extract the metadata for your measurements\n",
    "- upload the measurements to Delta Lake\n",
    "\n",
    "The process takes between 20 secs per gigabyte (*note: we can speed this up by parellizing the upload process*).\n",
    "\n",
    "### Example netCDF File\n",
    "An example netCDF file can be downloaded here: [https://figshare.com/ndownloader/files/50061330](https://figshare.com/ndownloader/files/50061330) that meets the above requirements and can be used as a template for your own data.\n",
    "\n",
    "Once you've downloaded that file into the local `DiveDB/files/` subdirectory, you'll either need to rename it to `example_data.nc` or set `example_data_path` in the following examples to the name of the downloaded file. \n",
    "\n",
    "### Example 1: Uploading a netCDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "import xarray as xr\n",
    "\n",
    "from DiveDB.services.duck_pond import DuckPond\n",
    "\n",
    "import DiveDB.services.data_uploader\n",
    "importlib.reload(DiveDB.services.data_uploader)\n",
    "from DiveDB.services.data_uploader import DataUploader\n",
    "\n",
    "# Create DuckPond instance (new Iceberg-based data lake)\n",
    "duck_pond = DuckPond(os.environ[\"CONTAINER_ICEBERG_PATH\"])\n",
    "data_uploader = DataUploader(duck_pond=duck_pond)\n",
    "\n",
    "# See above [Example netCDF File](#example-netcdf-file) for saving an example file \n",
    "# to this path; if the file has not been renamed to `example_data.nc`, update the \n",
    "# path this variable points to.\n",
    "example_data_path = \"/Users/williamgislason/Downloads/2025-01-30_oror-001_output.nc\"\n",
    "\n",
    "# Prepare data for each model\n",
    "with xr.open_dataset(example_data_path) as ds:\n",
    "    display(ds)\n",
    "    dataset_id = ds.attrs.get(\"dataset_info_page_id\")\n",
    "    animal_id = ds.attrs.get(\"animal_info_page_id\")\n",
    "    deployment_id = ds.attrs.get(\"deployment_info_page_id\")\n",
    "    \n",
    "    sensor_info_attrs = {key: value for key, value in ds.attrs.items() if key.startswith(\"sensor_info\")}\n",
    "    sensor_info_words = list(set(key.split(\"sensor_info_\")[1].split(\"_\")[0] for key in sensor_info_attrs))\n",
    "    logger_ids = {ds.attrs.get(f\"sensor_info_{word}_logger_id\") for word in sensor_info_words}\n",
    "    \n",
    "    if len(logger_ids) == 1:\n",
    "        logger_id = list(logger_ids)[0]\n",
    "        metadata = {\n",
    "            \"dataset\": dataset_id,\n",
    "            \"animal\": animal_id,\n",
    "            \"deployment\": deployment_id,\n",
    "            \"recording\": f\"{deployment_id}_{animal_id}_{logger_id}\"\n",
    "        }\n",
    "\n",
    "        data_uploader.upload_netcdf(example_data_path, metadata)\n",
    "    else:\n",
    "        print(\"Multiple loggers detected. Divide data into separate files for each logger.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DiveDB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
