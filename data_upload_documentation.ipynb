{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Uploader\n",
    "\n",
    "This notebook demonstrates the process of uploading EDF files data to Delta Lake and OpenStack Swift for long-term storage. \n",
    "\n",
    "It also includes the setup and execution of the data upload process, as well as querying the uploaded data for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the servers:\n",
    "To launch the server, open the Docker Desktop app and run the following command at the root of the project:\n",
    "```bash\n",
    "$ make up\n",
    "```\n",
    "This command will launch the Django server, Postgres database, and Jupyter server using the environment variables defined in the `.env` file accross all containers.\n",
    "\n",
    "#### Understanding expected file paths:\n",
    "DiveDB expects the following paths to be set in the `.env` file:\n",
    "- `CONTAINER_DATA_PATH`\n",
    "- `LOCAL_DATA_PATH`\n",
    "- `HOST_DELTA_LAKE_PATH`\n",
    "- `CONTAINER_DELTA_LAKE_PATH`\n",
    "- `HOST_FILE_STORAGE_PATH`\n",
    "- `CONTAINER_FILE_STORAGE_PATH`\n",
    "\n",
    "These paths are used to mount the Delta Lake and file storage to the containers. The \"LOCAL_\" and \"HOST_\" paths can be wherever makes sense for your local machine. The \"CONTAINER_\" paths are the paths that the containers expect. We recommend you keep the \"CONTAINER_\" paths as they are in the `.env.example` file.\n",
    "\n",
    "#### When is the server ready?\n",
    "There are 3 processes that need to be running for the server to be ready:\n",
    "1. The Django server (`web`)\n",
    "2. The Postgres database (`metadata_database`)\n",
    "3. The Jupyter server (`jupyter`)\n",
    "\n",
    "Jupyter is almost always the last to start up. You'll know it's ready when you see the following logs in the terminal:\n",
    "```bash\n",
    "jupyter-1            | [I 2024-08-30 16:12:37.083 ServerApp] Serving notebooks from local directory: /app\n",
    "jupyter-1            | [I 2024-08-30 16:12:37.083 ServerApp] Jupyter Server 2.14.2 is running at:\n",
    "jupyter-1            | [I 2024-08-30 16:12:37.083 ServerApp] http://e29d05e13fd0:8888/jupyter/tree\n",
    "jupyter-1            | [I 2024-08-30 16:12:37.083 ServerApp]     http://127.0.0.1:8888/jupyter/tree\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to the Jupyter Kernel:\n",
    "To connect to the Jupyter server in your notebook, follow these steps:\n",
    "1. Click the \"Select Kernel\" button at the top right of the page.\n",
    "1. Pick the \"Select another kernel\" option in the dropdown menu.\n",
    "1. Pick the \"Existing Jupyter Server\" option in the dropdown menu.\n",
    "1. Now we need to connect to the Jupyter server.\n",
    "    - If you previously connected to the Jupyter server\n",
    "        - Pick the \"localhost\" option in the dropdown menu (or whatever you named it prior)\n",
    "    - If you have not connected to the Jupyter server before\n",
    "        - Pick the \"Enter the URL of the running Jupyter server\" option in the dropdown menu.\n",
    "        - Enter http://localhost:8888/jupyter\n",
    "        - Give it a name you'll remember (like \"Local DiveDB Jupyter Server\")\n",
    "1. Press the \"Reload\" icon in the top right of the dropdown menu to see the latest kernel.\n",
    "1. Pick the \"Python 3\" option in the dropdown menu.\n",
    "\n",
    "This will ensure you execute the Jupyter notebook in the correct environment.\n",
    "\n",
    "After connecting to the Jupyter server, ensure your notebook runs this command to set the appropriate file paths:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing to upload data:\n",
    "There are two aspects to any data upload:\n",
    "1. A table containing measurements and time\n",
    "    - This can be an EDF, CSV, or a data\n",
    "2. The metadata for the measurements\n",
    "    - This describes the context of the measurements using the following fields:\n",
    "        - animal\n",
    "        - deployment\n",
    "        - logger\n",
    "        - recording\n",
    "\n",
    "There are several ways to define your metadata. \n",
    "\n",
    "#### Option 1: Supplied Metadata CSV File\n",
    "If you have a metadata file, you can use the `metadata_map` dictionary to map the columns in the CSV file to the corresponding mode. You can then pass the metadata file to the `upload_edf` function which will interactively extract the metadata for your measurements using the data in the metadata file and in the Metadata Database.\n",
    "\n",
    "#### Option 2: Interactive Selection (in the works)\n",
    "If you provide no metadata, you can use the `upload_edf` function to interactively select the metadata for your measurements using the data in the Metadata Database.\n",
    "\n",
    "#### Option 3: Supplied Metadata Dictionary\n",
    "If you know the metadata for your measurements, you can pass a dictionary to the `upload_edf` function. The dictionary should represent metadata existing in the Metadata database and contain the following fields:\n",
    "- animal: The animal ID\n",
    "- deployment: The deployment name\n",
    "- logger: The logger ID\n",
    "- recording: The recording name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading EDFs\n",
    "The `edf_file_paths` list contains the paths to the EDF files that we want to upload. It can point to files on your local machine or on a remote server.\n",
    "In this example, the files are located in the ../data/files/ directory and are named test12_Wednesday_05_DAY1_PROCESSED.edf and test12_Wednesday_05_DAY2_PROCESSED.edf.\n",
    "\n",
    "The `metadata_file_path` variable holds the path to the CSV file containing metadata for the EDF files (named Sleep Study Metadata.csv). \n",
    "\n",
    "The `metadata_map` dictionary is used to map the columns in the CSV metadata file to the corresponding mode. The keys in the dictionary represent the fields in the database, and the values represent the column names in the CSV file. For example:\n",
    "- \"animal\" maps to the \"Nickname\" column in the CSV file.\n",
    "- \"deployment\" maps to the \"Deployment\" column in the CSV file.\n",
    "- \"logger\" maps to the \"Logger Used\" column in the CSV file.\n",
    "- \"recording\" maps to the \"Recording ID\" column in the CSV file.\n",
    "\n",
    "The upload_edf function will perform the following: \n",
    "- use the metadata map to extract the metadata for your measurements using the data in the metadata file and in the Metadata Database\n",
    "- upload copies of the EDF files to OpenStack Swift\n",
    "- upload the measurements to Delta Lake (by default, 10M measurements per batch)\n",
    "\n",
    "The process takes between 5-10 minutes to complete per gigabyte. (*note: we can speed this up by parellizing the upload process*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"\n",
    "\n",
    "import importlib\n",
    "import DiveDB.services.data_uploader\n",
    "importlib.reload(DiveDB.services.data_uploader)\n",
    "from DiveDB.services.data_uploader import DataUploader\n",
    "\n",
    "\n",
    "data_uploader = DataUploader()\n",
    "\n",
    "\n",
    "edf_file_paths = [\n",
    "    \"./data/files/test12_Wednesday_05_DAY1_PROCESSED.edf\",\n",
    "    \"./data/files/test12_Wednesday_05_DAY2_PROCESSED.edf\"\n",
    "]\n",
    "\n",
    "metadata_file_path = \"./data/files/Sleep Study Metadata.csv\"\n",
    "\n",
    "metadata_map = {\n",
    "    \"animal\": \"Nickname\",\n",
    "    \"deployment\": \"Deployment\",\n",
    "    \"logger\": \"Logger Used\",\n",
    "    \"recording\": \"Recording ID\"\n",
    "}\n",
    "\n",
    "data_uploader.upload_edf(edf_file_paths, metadata_file_path, metadata_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading NetCDF files\n",
    "The `netcdf_file_path` list contains the paths to the NetCDF files that we want to upload. It can point to files on your local machine or on a remote server.\n",
    "In this example, the file is located in the ../data/files/ directory and is named deployment_data.nc.\n",
    "\n",
    "The upload_netcdf function will perform the following: \n",
    "- use the provided metadata dictionary to extract the metadata for your measurements\n",
    "- upload copies of the NetCDF files to OpenStack Swift\n",
    "- upload the measurements to Delta Lake\n",
    "\n",
    "The process takes between 30 secs to 1 min to complete per gigabyte — about 2/3rds of the time is used to upload the files to OpenStack Swift. (*note: we can speed this up by parellizing the upload process*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating file record for deployment_data.nc and uploading to OpenStack...\n",
      "Processing 10 variables in the netCDF file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing variables:   0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Generic S3 error: Error after 0 retries in 137.5µs, max_retries:10, retry_timeout:180s, source:builder error for url (http://localhost:9000/divedb-delta-lakes/data/_delta_log/_last_checkpoint)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m     10\u001b[0m data_uploader \u001b[38;5;241m=\u001b[39m DataUploader()\n\u001b[1;32m     12\u001b[0m metadata \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manimal\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moror-002\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeployment\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2024-01-16_oror-002a\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecording\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2024-01-16_oror-002a_UF-01_001\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     16\u001b[0m }\n\u001b[0;32m---> 19\u001b[0m \u001b[43mdata_uploader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_netcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data/deployment_data.nc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/app/DiveDB/services/data_uploader.py:385\u001b[0m, in \u001b[0;36mDataUploader.upload_netcdf\u001b[0;34m(self, netcdf_file_path, metadata, batch_size)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;66;03m# Create the batch table\u001b[39;00m\n\u001b[1;32m    362\u001b[0m batch_table \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39mtable(\n\u001b[1;32m    363\u001b[0m     {\n\u001b[1;32m    364\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msignal_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: pa\u001b[38;5;241m.\u001b[39marray(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     schema\u001b[38;5;241m=\u001b[39mLAKE_CONFIGS[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATA\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mschema\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    383\u001b[0m )\n\u001b[0;32m--> 385\u001b[0m \u001b[43mduckpond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_to_delta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlake\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDATA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manimal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeployment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecording\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msignal_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_labels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# file.file.name,\u001b[39;49;00m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m batch_table\n\u001b[1;32m    400\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[0;32m/app/DiveDB/services/duck_pond.py:155\u001b[0m, in \u001b[0;36mDuckPond.write_to_delta\u001b[0;34m(self, data, lake, mode, partition_by, name, description, schema_mode)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m    154\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mDEBUG)\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta_lake \u001b[38;5;241m=\u001b[39m \u001b[43mwrite_deltalake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtable_or_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLAKE_CONFIGS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlake\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLAKE_CONFIGS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlake\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mschema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_by\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAWS_ENDPOINT_URL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttp://localhost:9000\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAWS_ACCESS_KEY_ID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOPENSTACK_APPLICATION_CREDENTIAL_ID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAWS_SECRET_ACCESS_KEY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOPENSTACK_APPLICATION_CREDENTIAL_SECRET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAWS_REGION\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mus-east-1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAWS_S3_URL_STYLE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_lake_views(lake)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/deltalake/writer.py:281\u001b[0m, in \u001b[0;36mwrite_deltalake\u001b[0;34m(table_or_uri, data, schema, partition_by, mode, file_options, max_partitions, max_open_files, max_rows_per_file, min_rows_per_group, max_rows_per_group, name, description, configuration, schema_mode, storage_options, partition_filters, predicate, target_file_size, large_dtypes, engine, writer_properties, custom_metadata, post_commithook_properties)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite_deltalake\u001b[39m(\n\u001b[1;32m    191\u001b[0m     table_or_uri: Union[\u001b[38;5;28mstr\u001b[39m, Path, DeltaTable],\n\u001b[1;32m    192\u001b[0m     data: Union[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    223\u001b[0m     post_commithook_properties: Optional[PostCommitHookProperties] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    224\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Write to a Delta Lake table\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m    If the table does not already exist, it will be created.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m        post_commithook_properties: properties for the post commit hook. If None, default values are used.\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m     table, table_uri \u001b[38;5;241m=\u001b[39m \u001b[43mtry_get_table_and_table_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable_or_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m table \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    283\u001b[0m         storage_options \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39m_storage_options \u001b[38;5;129;01mor\u001b[39;00m {}\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/deltalake/writer.py:742\u001b[0m, in \u001b[0;36mtry_get_table_and_table_uri\u001b[0;34m(table_or_uri, storage_options)\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable_or_uri must be a str, Path or DeltaTable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(table_or_uri, (\u001b[38;5;28mstr\u001b[39m, Path)):\n\u001b[0;32m--> 742\u001b[0m     table \u001b[38;5;241m=\u001b[39m \u001b[43mtry_get_deltatable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable_or_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    743\u001b[0m     table_uri \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(table_or_uri)\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/deltalake/writer.py:755\u001b[0m, in \u001b[0;36mtry_get_deltatable\u001b[0;34m(table_uri, storage_options)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtry_get_deltatable\u001b[39m(\n\u001b[1;32m    752\u001b[0m     table_uri: Union[\u001b[38;5;28mstr\u001b[39m, Path], storage_options: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]\n\u001b[1;32m    753\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[DeltaTable]:\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 755\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDeltaTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    756\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TableNotFoundError:\n\u001b[1;32m    757\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/deltalake/table.py:380\u001b[0m, in \u001b[0;36mDeltaTable.__init__\u001b[0;34m(self, table_uri, version, storage_options, without_files, log_buffer_size)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03mCreate the Delta Table from a path with an optional version.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03mMultiple StorageBackends are currently supported: AWS S3, Azure Data Lake Storage Gen2, Google Cloud Storage (GCS) and local URI.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    377\u001b[0m \n\u001b[1;32m    378\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage_options \u001b[38;5;241m=\u001b[39m storage_options\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_table \u001b[38;5;241m=\u001b[39m \u001b[43mRawDeltaTable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtable_uri\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwithout_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwithout_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_buffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_buffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOSError\u001b[0m: Generic S3 error: Error after 0 retries in 137.5µs, max_retries:10, retry_timeout:180s, source:builder error for url (http://localhost:9000/divedb-delta-lakes/data/_delta_log/_last_checkpoint)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import importlib\n",
    "\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"\n",
    "\n",
    "import DiveDB.services.data_uploader\n",
    "importlib.reload(DiveDB.services.data_uploader)\n",
    "from DiveDB.services.data_uploader import DataUploader\n",
    "\n",
    "data_uploader = DataUploader()\n",
    "\n",
    "metadata = {\n",
    "    'animal': 'oror-002', \n",
    "    'deployment': '2024-01-16_oror-002a', \n",
    "    'recording': '2024-01-16_oror-002a_UF-01_001'\n",
    "}\n",
    "\n",
    "\n",
    "data_uploader.upload_netcdf(\"./data/deployment_data.nc\", metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (InvalidBucketName) when calling the ListBuckets operation: The specified bucket is not valid.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m\n\u001b[1;32m      5\u001b[0m s3 \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mclient(\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     aws_access_key_id\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENSTACK_APPLICATION_CREDENTIAL_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     )\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# List objects in the bucket\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43ms3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_buckets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMaxBuckets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/botocore/client.py:569\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    566\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/botocore/client.py:1023\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (InvalidBucketName) when calling the ListBuckets operation: The specified bucket is not valid."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=os.getenv(\"OPENSTACK_APPLICATION_CREDENTIAL_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"OPENSTACK_APPLICATION_CREDENTIAL_SECRET\"),\n",
    "    endpoint_url='https://object.cloud.sdsc.edu:443/v1/AUTH_413c350724914abbbb2ece619b2b69d4',\n",
    "    region_name='us-east-1',\n",
    "    config=Config(\n",
    "        s3={'addressing_style': 'path'},\n",
    "        signature_version='s3',\n",
    "    )\n",
    ")\n",
    "\n",
    "# List objects in the bucket\n",
    "response = s3.list_buckets( )\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
