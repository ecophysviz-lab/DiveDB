{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Querying and Exporting\n",
    "\n",
    "This notebook demonstrates the process of querying data from Delta Lake and exporting it in various formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the servers:\n",
    "To launch the server, open the Docker Desktop app and run the following command at the root of the project:\n",
    "```bash\n",
    "$ make up\n",
    "```\n",
    "This command will launch the Django server, Postgres database, and Jupyter server using the environment variables defined in the `.env` file accross all containers.\n",
    "\n",
    "#### Understanding expected file paths:\n",
    "DiveDB expects the following paths to be set in the `.env` file:\n",
    "- `CONTAINER_DATA_PATH`\n",
    "- `LOCAL_DATA_PATH`\n",
    "- `HOST_DELTA_LAKE_PATH`\n",
    "- `CONTAINER_DELTA_LAKE_PATH`\n",
    "- `HOST_FILE_STORAGE_PATH`\n",
    "- `CONTAINER_FILE_STORAGE_PATH`\n",
    "\n",
    "These paths are used to mount the Delta Lake and file storage to the containers. The \"LOCAL_\" and \"HOST_\" paths can be wherever makes sense for your local machine. The \"CONTAINER_\" paths are the paths that the containers expect. We recommend you keep the \"CONTAINER_\" paths as they are in the `.env.example` file.\n",
    "\n",
    "#### When is the server ready?\n",
    "There are 3 processes that need to be running for the server to be ready:\n",
    "1. The Django server (`web`)\n",
    "2. The Postgres database (`metadata_database`)\n",
    "3. The Jupyter server (`jupyter`)\n",
    "\n",
    "Jupyter is almost always the last to start up. You'll know it's ready when you see the following logs in the terminal:\n",
    "```bash\n",
    "jupyter-1            | [I 2024-08-30 16:12:37.083 ServerApp] Serving notebooks from local directory: /app\n",
    "jupyter-1            | [I 2024-08-30 16:12:37.083 ServerApp] Jupyter Server 2.14.2 is running at:\n",
    "jupyter-1            | [I 2024-08-30 16:12:37.083 ServerApp] http://e29d05e13fd0:8888/jupyter/tree\n",
    "jupyter-1            | [I 2024-08-30 16:12:37.083 ServerApp]     http://127.0.0.1:8888/jupyter/tree\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to the Jupyter Kernel:\n",
    "To connect to the Jupyter server in your notebook, follow these steps:\n",
    "1. Click the \"Select Kernel\" button at the top right of the page.\n",
    "1. Pick the \"Select another kernel\" option in the dropdown menu.\n",
    "1. Pick the \"Existing Jupyter Server\" option in the dropdown menu.\n",
    "1. Now we need to connect to the Jupyter server.\n",
    "    - If you previously connected to the Jupyter server\n",
    "        - Pick the \"localhost\" option in the dropdown menu (or whatever you named it prior)\n",
    "    - If you have not connected to the Jupyter server before\n",
    "        - Pick the \"Enter the URL of the running Jupyter server\" option in the dropdown menu.\n",
    "        - Enter http://localhost:8888/jupyter\n",
    "        - Give it a name you'll remember (like \"Local DiveDB Jupyter Server\")\n",
    "1. Press the \"Reload\" icon in the top right of the dropdown menu to see the latest kernel.\n",
    "1. Pick the \"Python 3\" option in the dropdown menu.\n",
    "\n",
    "This will ensure you execute the Jupyter notebook in the correct environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying from Delta Lake\n",
    "We connect to our datastores using the `DuckPond` class. DuckPond is a wrapper around a DuckDB connection with access to both our Metadata Database and our measurements stored in Delta Lake. The ability to query both sources of data from a single connection is useful for quickly accessing data for analysis.\n",
    "\n",
    "There are two main ways to query data from Delta Lake:\n",
    "1. Using the DuckPond `get_delta_data` method\n",
    "2. Using the DuckPond connection to query directly\n",
    "\n",
    "### Using the DuckPond `get_delta_data` method\n",
    "DuckPond's `get_delta_data` method constructs a query based on the parameters you pass to it and returns a DuckDB DataFrame. It is useful for quickly accessing data for analysis. It takes the following optional parameters:\n",
    "- `signal_names`: A string or list of signal names to query.\n",
    "- `logger_ids`: A string or list of logger IDs to query.\n",
    "- `animal_ids`: A string or list of animal IDs to query.\n",
    "- `deployment_ids`: A string or list of deployment IDs to query.\n",
    "- `recording_ids`: A string or list of recording IDs to query.\n",
    "- `date_range`: A tuple of start and end dates to query.\n",
    "- `limit`: The maximum number of rows to return.\n",
    "\n",
    "The `get_delta_data` method returns a [DuckDB DuckDBPyConnection](https://duckdb.org/docs/api/python/reference/#duckdb.DuckDBPyConnection) which can be used to convert the data in many different formats including the following ([see documentation for a full list](https://duckdb.org/docs/api/python/conversion#result-conversion-duckdb-results-to-python))\n",
    "- NumPy Array (`.fetchnumpy()`)\n",
    "- Pandas DataFrame (`.df()`)\n",
    "- Arrows Table (`.arrow()`)\n",
    "- Polars DataFrame (`.pl()`)\n",
    "\n",
    "Until a conversion method is called, the data is not loaded into memory. This allows for large queries to be run without using too much memory.\n",
    "\n",
    "##### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the following query:\n",
      "SELECT datetime, values FROM DeltaLake WHERE signal_name = 'sensor_data_temperature'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mDiveDB\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01medf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_mne_array, create_mne_edf\n\u001b[1;32m     13\u001b[0m duckpond \u001b[38;5;241m=\u001b[39m DuckPond()\n\u001b[0;32m---> 15\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[43mduckpond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_delta_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m    \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43msignal_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msensor_data_temperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m display(conn)\n",
      "File \u001b[0;32m/app/DiveDB/services/duck_pond.py:166\u001b[0m, in \u001b[0;36mDuckPond.get_delta_data\u001b[0;34m(self, signal_names, logger_ids, animal_ids, deployment_ids, recording_ids, date_range, limit)\u001b[0m\n\u001b[1;32m    164\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconn\u001b[38;5;241m.\u001b[39msql(query_string)\n\u001b[1;32m    165\u001b[0m value_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(signal_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetchone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalue_index\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(signal_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconn\u001b[38;5;241m.\u001b[39msql(\n\u001b[1;32m    169\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124m            SELECT signal_name, datetime, unnest(values) as value\u001b[39m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124m            FROM results\u001b[39m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    173\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import DiveDB.services.duck_pond\n",
    "import DiveDB.services.utils.edf\n",
    "importlib.reload(DiveDB.services.duck_pond)\n",
    "importlib.reload(DiveDB.services.utils.edf)\n",
    "\n",
    "from DiveDB.services.duck_pond import DuckPond\n",
    "from DiveDB.services.utils.edf import create_mne_array, create_mne_edf\n",
    "\n",
    "duckpond = DuckPond()\n",
    "\n",
    "conn = duckpond.get_delta_data(    \n",
    "    signal_names=[\"sensor_data_temperature\"],\n",
    ")\n",
    "\n",
    "display(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the DuckPond connection to query directly\n",
    "More complex queries can be run directly on the DuckPond connection. This is useful for queries that may not be supported by the `get_delta_data` method which has those involving grouping or aggregations. \n",
    "\n",
    "DuckDB runs sql very similar in syntax to other SQL databases. A full breakdown of the syntax can be found [in the documenation](https://duckdb.org/docs/sql/introduction).\n",
    "\n",
    "The connection object can be found in the `duckpond.conn` attribute. To run queries, use the `sql` method which also returns a [DuckDB DuckDBPyConnection](https://duckdb.org/docs/api/python/reference/#duckdb.DuckDBPyConnection) which can be used to convert the data in many different formats including the following ([see documentation for a full list](https://duckdb.org/docs/api/python/conversion#result-conversion-duckdb-results-to-python))\n",
    "- NumPy Array (`.fetchnumpy()`)\n",
    "- Pandas DataFrame (`.df()`)\n",
    "- Arrows Table (`.arrow()`)\n",
    "- Polars DataFrame (`.pl()`)\n",
    "\n",
    "##### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "# Reload the DuckPond module to pick up any changes\n",
    "import DiveDB.services.duck_pond\n",
    "importlib.reload(src.DiveDB.services.duck_pond)\n",
    "from DiveDB.services.duck_pond import DuckPond\n",
    "\n",
    "duckpond = DuckPond()\n",
    "\n",
    "df = duckpond.conn.sql(f\"\"\"\n",
    "SELECT logger, signal_name, avg(value) as mean_data\n",
    "FROM (\n",
    "    SELECT logger, signal_name, unnest(values) as value\n",
    "    FROM DeltaLake\n",
    "    WHERE signal_name = 'sensor_data_temperature'\n",
    ")\n",
    "GROUP BY logger, signal_name\n",
    "\"\"\").df()\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining Queries\n",
    "Queries can be chained together to form a pipeline. This is useful for running complex queries that involve multiple steps.\n",
    "\n",
    "##### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "# Reload the DuckPond module to pick up any changes\n",
    "import DiveDB.services.duck_pond\n",
    "importlib.reload(DiveDB.services.duck_pond)\n",
    "from DiveDB.services.duck_pond import DuckPond\n",
    "\n",
    "# Get the filtered data\n",
    "filtered_data = duckpond.get_delta_data(    \n",
    "    logger_ids=\"NL-01\",\n",
    "    animal_ids=\"mian-001\",\n",
    "    signal_names=[\"sensor_data_light\", \"sensor_data_temperature\"],\n",
    ")\n",
    "\n",
    "# Perform the aggregation on the filtered data\n",
    "df = duckpond.conn.sql(\"\"\"\n",
    "SELECT signal_name, max(value) as max_data\n",
    "FROM filtered_data -- NOTE: filtered_data is our DuckDBPyConnection object from the previous query\n",
    "GROUP BY signal_name\n",
    "\"\"\").df()\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Variables\n",
    "Sometimes we don't want to hardcode variables in our queries. We can use the `execute` method to pass variables to the query.\n",
    "\n",
    "##### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "# Reload the DuckPond module to pick up any changes\n",
    "import DiveDB.services.duck_pond\n",
    "importlib.reload(DiveDB.services.duck_pond)\n",
    "from DiveDB.services.duck_pond import DuckPond\n",
    "\n",
    "duckpond = DuckPond()\n",
    "\n",
    "signal_name = \"sensor_data_temperature\"\n",
    "df = duckpond.conn.execute(f\"\"\"\n",
    "SELECT logger, signal_name, avg(value) as mean_data\n",
    "FROM (\n",
    "    SELECT logger, signal_name, unnest(values) as value\n",
    "    FROM DeltaLake\n",
    "    WHERE signal_name = $1\n",
    ")\n",
    "GROUP BY logger, signal_name\n",
    "\"\"\", [signal_name]).df()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Metadata Database\n",
    "We can also query the Metadata Database directly. This is useful for querying data that is not stored in Delta Lake and joining it for queries on measurement data.\n",
    "\n",
    "##### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "# Reload the DuckPond module to pick up any changes\n",
    "import DiveDB.services.duck_pond\n",
    "importlib.reload(DiveDB.services.duck_pond)\n",
    "from DiveDB.services.duck_pond import DuckPond\n",
    "\n",
    "duckpond = DuckPond()\n",
    "\n",
    "\n",
    "# Show all tables we have access to\n",
    "print(duckpond.get_db_schema())\n",
    "\n",
    "df = duckpond.conn.sql(\"\"\"\n",
    "SELECT unnest(values) as value\n",
    "FROM DeltaLake \n",
    "JOIN Metadata.public.Animals ON DeltaLake.animal = Animals.id\n",
    "WHERE Animals.project_id = 'test12_Wednesday'\n",
    "AND signal_name = 'sensor_data_temperature'\n",
    "\"\"\").df()\n",
    "\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Data to EDF\n",
    "When it's easier to work with EDF files, we can export the data to an EDF file. This is useful for working with the data in other software packages.\n",
    "\n",
    "The `create_mne_edf` function takes a DuckDB connection and a file path and creates an EDF file. \n",
    "\n",
    "*Note: it currently requires a lot of memory. Can be improved.*\n",
    "*Note: it's lacking support for most info fields in the EDF file. Can be improved.*\n",
    "\n",
    "##### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "import DiveDB.services.duck_pond\n",
    "import DiveDB.services.utils.edf\n",
    "importlib.reload(DiveDB.services.duck_pond)\n",
    "importlib.reload(DiveDB.services.utils.edf)\n",
    "\n",
    "from DiveDB.services.duck_pond import DuckPond\n",
    "from DiveDB.services.utils.edf import create_mne_edf\n",
    "\n",
    "duckpond = DuckPond()\n",
    "\n",
    "conn = duckpond.get_delta_data(    \n",
    "    logger_ids=\"NL-02\",\n",
    "    animal_ids=\"mian-003\",\n",
    "    signal_names=[\"ECG_ICA2\", \"EEG_ICA5\"],\n",
    "    limit=1000000,\n",
    ")\n",
    "\n",
    "create_mne_edf(conn, \"test.edf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Data to MNE Signal Array\n",
    "For working with the data in MNE, we can export the data to an MNE Signal Array. This is useful for manipulating the data in MNE.\n",
    "\n",
    "The `create_mne_array` function takes a DuckDB connection and returns an MNE RawArray.\n",
    "\n",
    "##### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import DiveDB.services.duck_pond\n",
    "importlib.reload(DiveDB.services.duck_pond)\n",
    "from DiveDB.services.duck_pond import DuckPond\n",
    "from DiveDB.services.utils.edf import create_mne_array\n",
    "\n",
    "duckpond = DuckPond()\n",
    "\n",
    "conn = duckpond.get_delta_data(    \n",
    "    logger_ids=\"NL-02\",\n",
    "    animal_ids=\"mian-003\",\n",
    "    signal_names=\"ECG_ICA2\",\n",
    "    limit=1000000,\n",
    ")\n",
    "\n",
    "raw = create_mne_array(conn, resample=100, l_freq=1, h_freq=20)\n",
    "display(raw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
