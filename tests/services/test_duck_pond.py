"""
Test DuckPond Infrastructure (Iceberg-based) - with pytest

Tests generated by Claude 4 Sonnet.
"""

import tempfile
import pyarrow as pa
import pandas as pd
import numpy as np
import pytest

from DiveDB.services.duck_pond import DuckPond


@pytest.fixture
def temp_warehouse():
    """Create a temporary directory for testing"""
    with tempfile.TemporaryDirectory() as temp_dir:
        yield temp_dir


@pytest.fixture
def duck_pond(temp_warehouse):
    """Create an DuckPond instance for testing"""
    return DuckPond(warehouse_path=temp_warehouse, datasets=["test_dataset"])


@pytest.fixture
def test_schema():
    """Create the PyArrow schema matching DuckPond's Iceberg schema"""
    return pa.schema(
        [
            pa.field("dataset", pa.string(), nullable=False),  # Required
            pa.field("animal", pa.string(), nullable=False),  # Required
            pa.field("deployment", pa.string(), nullable=False),  # Required
            pa.field("recording", pa.string(), nullable=True),  # Optional
            pa.field("group", pa.string(), nullable=False),  # Required
            pa.field("class", pa.string(), nullable=False),  # Required
            pa.field("label", pa.string(), nullable=False),  # Required
            pa.field("datetime", pa.timestamp("us"), nullable=False),  # Required
            pa.field("val_dbl", pa.float64(), nullable=True),  # Optional
            pa.field("val_int", pa.int64(), nullable=True),  # Optional
            pa.field("val_bool", pa.bool_(), nullable=True),  # Optional
            pa.field("val_str", pa.string(), nullable=True),  # Optional
            pa.field("data_type", pa.string(), nullable=False),  # Required
        ]
    )


@pytest.fixture
def sample_data(test_schema):
    """Create sample test data with new dataset field"""
    return pa.table(
        [
            pa.array(["test_dataset"]),  # dataset
            pa.array(["seal_001"]),  # animal
            pa.array(["deploy_001"]),  # deployment
            pa.array(["rec_001"]),  # recording
            pa.array(["signal_data"]),  # group
            pa.array(["accelerometer"]),  # class
            pa.array(["acc_x"]),  # label
            pa.array([pd.Timestamp("2024-01-01T00:00:00")]),  # datetime
            pa.array([1.23]),  # val_dbl (has value)
            pa.array([None], type=pa.int64()),  # val_int (null)
            pa.array([None], type=pa.bool_()),  # val_bool (null)
            pa.array([None], type=pa.string()),  # val_str (null)
            pa.array(["double"]),  # data_type (indicates val_dbl has data)
        ],
        schema=test_schema,
    )


@pytest.fixture
def sample_int_data(test_schema):
    """Create sample test data with int value for testing data type conversion"""
    return pa.table(
        [
            pa.array(["test_dataset"]),  # dataset
            pa.array(["seal_002"]),  # animal
            pa.array(["deploy_002"]),  # deployment
            pa.array(["rec_002"]),  # recording
            pa.array(["signal_data"]),  # group
            pa.array(["accelerometer"]),  # class
            pa.array(["acc_y"]),  # label
            pa.array([pd.Timestamp("2024-01-01T01:00:00")]),  # datetime
            pa.array([None], type=pa.float64()),  # val_dbl (null)
            pa.array([42]),  # val_int (has value)
            pa.array([None], type=pa.bool_()),  # val_bool (null)
            pa.array([None], type=pa.string()),  # val_str (null)
            pa.array(["int"]),  # data_type (indicates val_int has data)
        ],
        schema=test_schema,
    )


class TestDuckPondInfrastructure:
    """Test Step 1.1: Basic DuckPond Infrastructure"""

    def test_duck_pond_creation(self, duck_pond):
        """Test that DuckPond can be created successfully"""
        assert duck_pond is not None
        assert duck_pond.catalog is not None
        assert duck_pond.conn is not None

    def test_duckdb_connection(self, duck_pond):
        """Test that DuckDB connection works"""
        result = duck_pond.conn.execute("SELECT 1 as test").fetchone()
        assert result[0] == 1

    def test_iceberg_extension_loaded(self, duck_pond):
        """Test that Iceberg extension is loaded"""
        extensions = duck_pond.conn.execute(
            "SELECT * FROM duckdb_extensions() WHERE extension_name = 'iceberg'"
        ).fetchall()
        assert len(extensions) > 0

    def test_views_created(self, duck_pond):
        """Test that expected dataset-specific views are created"""
        tables = duck_pond.get_db_schema().df()
        expected_views = [
            "test_dataset_Data",
            "test_dataset_Events",
        ]

        actual_views = tables[tables["database"] == "memory"]["name"].tolist()

        for view_name in expected_views:
            assert view_name in actual_views, f"Missing view: {view_name}"

    def test_schema_includes_dataset_field(self, duck_pond):
        """Test that the new dataset field is included in the schema"""
        data_table = duck_pond.catalog.load_table("test_dataset.data")
        schema = data_table.schema()

        # Check that dataset is field 1 and is required
        dataset_field = schema.find_field(1)
        assert dataset_field.name == "dataset"
        assert dataset_field.required is True

        # Check that animal is field 2
        animal_field = schema.find_field(2)
        assert animal_field.name == "animal"
        assert animal_field.required is True

    def test_write_data_with_dataset(self, duck_pond, sample_data):
        """Test writing data with the new dataset field"""
        duck_pond.write_to_iceberg(sample_data, "data", dataset="test_dataset")

        # Verify data was written by checking dataset-specific view
        result = duck_pond.conn.execute(
            'SELECT dataset, value FROM "test_dataset_Data" LIMIT 1'
        ).fetchone()
        assert result[0] == "test_dataset"
        assert result[1] == 1.23

    def test_dataset_specific_view(self, duck_pond, sample_data):
        """Test that dataset-specific view works correctly"""
        duck_pond.write_to_iceberg(sample_data, "data", dataset="test_dataset")

        result = duck_pond.conn.execute(
            'SELECT dataset, animal, value FROM "test_dataset_Data" LIMIT 1'
        ).fetchone()
        assert result[0] == "test_dataset"
        assert result[1] == "seal_001"
        assert result[2] == 1.23

    def test_wide_schema_columns_accessible(self, duck_pond, sample_data):
        """Test that wide schema columns are accessible"""
        duck_pond.write_to_iceberg(sample_data, "data", dataset="test_dataset")

        result = duck_pond.conn.execute(
            'SELECT float_value FROM "test_dataset_Data" LIMIT 1'
        ).fetchone()
        assert result[0] == 1.23

    def test_partitioning_excludes_dataset(self, duck_pond):
        """Test that partitioning no longer includes dataset (since it's the namespace)"""
        data_table = duck_pond.catalog.load_table("test_dataset.data")
        partition_spec = data_table.spec()

        # Should have 4 partition fields: animal, deployment, class, label (dataset is now namespace)
        assert len(partition_spec.fields) == 4

        # Check partition field names
        partition_names = [field.name for field in partition_spec.fields]
        assert (
            "dataset" not in partition_names
        )  # Dataset is no longer a partition field
        assert "animal" in partition_names
        assert "deployment" in partition_names
        assert "class" in partition_names
        assert "label" in partition_names

    def test_close_connection(self, duck_pond):
        """Test that connection can be closed cleanly"""
        duck_pond.close_connection()
        # Connection should be closed, but we can't easily test this without side effects
        # This mainly tests that the method exists and doesn't throw


class TestDuckPondDataTransformation:
    """Test Step 1.2: Data Transformation Functions (_create_wide_values)"""

    @pytest.fixture
    def mixed_values(self):
        """Create mixed-type test data"""
        import numpy as np

        # Use list instead of numpy array to preserve original types
        return [
            1.23,  # float
            42,  # int
            True,  # bool
            "test_string",  # string
            False,  # bool (false)
            np.nan,  # null/NaN
            3.14159,  # another float
            0,  # int (zero)
            None,  # explicit None
            "another_str",  # another string
        ]

    def test_create_wide_values_basic(self, duck_pond, mixed_values):
        """Test basic functionality of _create_wide_values"""
        val_dbl, val_int, val_bool, val_str, data_type = duck_pond._create_wide_values(
            mixed_values
        )

        # Check that all arrays have the same length
        assert len(val_dbl) == len(mixed_values)
        assert len(val_int) == len(mixed_values)
        assert len(val_bool) == len(mixed_values)
        assert len(val_str) == len(mixed_values)
        assert len(data_type) == len(mixed_values)

    def test_create_wide_values_float_detection(self, duck_pond):
        """Test float value detection and placement"""
        import numpy as np

        # Use list instead of numpy array to preserve original types
        values = [1.23, 3.14, np.inf, -2.5]

        val_dbl, val_int, val_bool, val_str, data_type = duck_pond._create_wide_values(
            values
        )

        # Check float values (excluding inf which should go to string)
        assert val_dbl[0].as_py() == 1.23
        assert val_dbl[1].as_py() == 3.14
        assert val_dbl[3].as_py() == -2.5

        # Check data types
        assert data_type[0].as_py() == "double"
        assert data_type[1].as_py() == "double"
        assert data_type[3].as_py() == "double"

        # inf should be treated as string
        assert data_type[2].as_py() == "str"

    def test_create_wide_values_int_detection(self, duck_pond):
        """Test integer value detection and placement"""
        import numpy as np

        # Use list instead of numpy array to preserve original types
        values = [42, 0, -17, np.int64(100)]

        val_dbl, val_int, val_bool, val_str, data_type = duck_pond._create_wide_values(
            values
        )

        # Check int values
        assert val_int[0].as_py() == 42
        assert val_int[1].as_py() == 0
        assert val_int[2].as_py() == -17
        assert val_int[3].as_py() == 100

        # Check data types
        for i in range(4):
            assert data_type[i].as_py() == "int"

    def test_create_wide_values_bool_detection(self, duck_pond):
        """Test boolean value detection and placement"""
        import numpy as np

        # Use list instead of numpy array to preserve original types
        values = [True, False, np.bool_(True)]

        val_dbl, val_int, val_bool, val_str, data_type = duck_pond._create_wide_values(
            values
        )

        # Check bool values
        assert val_bool[0].as_py() is True
        assert val_bool[1].as_py() is False
        assert val_bool[2].as_py() is True

        # Check data types
        for i in range(3):
            assert data_type[i].as_py() == "bool"

    def test_create_wide_values_string_detection(self, duck_pond):
        """Test string value detection and placement"""

        # Use list instead of numpy array to preserve original types
        values = ["test", "another", "", "123abc"]

        val_dbl, val_int, val_bool, val_str, data_type = duck_pond._create_wide_values(
            values
        )

        # Check string values
        assert val_str[0].as_py() == "test"
        assert val_str[1].as_py() == "another"
        assert val_str[2].as_py() == ""
        assert val_str[3].as_py() == "123abc"

        # Check data types
        for i in range(4):
            assert data_type[i].as_py() == "str"

    def test_create_wide_values_null_handling(self, duck_pond):
        """Test null/NaN value handling"""

        # Use list instead of numpy array to preserve original types
        values = [None, np.nan, 1.0, None]

        val_dbl, val_int, val_bool, val_str, data_type = duck_pond._create_wide_values(
            values
        )

        # Check null handling
        assert val_dbl[0].as_py() is None
        assert val_int[0].as_py() is None
        assert val_bool[0].as_py() is None
        assert val_str[0].as_py() is None
        assert data_type[0].as_py() == "null"

        # NaN should also be treated as null
        assert data_type[1].as_py() == "null"

        # Valid value should work
        assert val_dbl[2].as_py() == 1.0
        assert data_type[2].as_py() == "double"

    def test_write_sensor_data_complete_workflow(self, duck_pond):
        """Test the complete sensor data writing workflow"""
        import pandas as pd

        # Create test data
        dataset = "test_dataset_2024"
        metadata = {
            "animal": "seal_003",
            "deployment": "deploy_003",
            "recording": "rec_003",
        }
        times = pa.array([pd.Timestamp("2024-01-15T12:00:00")] * 5)
        # Use list instead of numpy array to preserve original types
        values = [1.5, 10, True, "sensor_error", False]

        # Write data
        rows_written = duck_pond.write_signal_data(
            dataset=dataset,
            metadata=metadata,
            times=times,
            group="sensor_data",
            class_name="temperature",
            label="temp_c",
            values=values,
        )

        # Verify write succeeded
        assert rows_written == 5

        # Verify data can be queried using dataset-specific view
        result = duck_pond.conn.execute(
            'SELECT COUNT(*) FROM "test_dataset_2024_Data"'
        ).fetchone()
        assert result[0] == 5

        # Verify data types are correctly stored
        results = duck_pond.conn.execute(
            """
            SELECT float_value, int_value, boolean_value, string_value, value
            FROM "test_dataset_2024_Data"
            ORDER BY datetime
        """
        ).fetchall()

        # Check that values are in the right columns and the 'value' column works
        assert results[0][0] == 1.5  # float_value
        assert results[0][4] == 1.5  # value (should equal float_value)

        assert results[1][1] == 10  # int_value
        assert results[1][4] == 10.0  # value (int converted to double)

        assert results[2][2] is True  # boolean_value
        assert results[2][4] == 1.0  # value (bool converted to double)

        assert results[3][3] == "sensor_error"  # string_value
        assert results[3][4] is None  # value (string can't convert to double)

    def test_write_sensor_data_with_dataset_partitioning(self, duck_pond):
        """Test that dataset partitioning works correctly"""
        import pandas as pd

        # Write data to different datasets
        for dataset_name in ["dataset_A", "dataset_B"]:
            metadata = {"animal": "seal_001", "deployment": "deploy_001"}
            times = pa.array([pd.Timestamp("2024-01-01T00:00:00")])
            # Use list instead of numpy array to preserve original types
            values = [1.0]

            duck_pond.write_signal_data(
                dataset=dataset_name,
                metadata=metadata,
                times=times,
                group="test",
                class_name="test",
                label="test",
                values=values,
            )

        # Verify both datasets are present using dataset-specific views
        result_a = duck_pond.conn.execute(
            'SELECT COUNT(*) FROM "dataset_A_Data"'
        ).fetchone()
        result_b = duck_pond.conn.execute(
            'SELECT COUNT(*) FROM "dataset_B_Data"'
        ).fetchone()

        assert result_a[0] == 1
        assert result_b[0] == 1

    def test_helper_methods(self, duck_pond):
        """Test the new helper methods for view names"""
        dataset = "EP Physiology"

        # Ensure dataset is initialized
        duck_pond.ensure_dataset_initialized(dataset)

        # Test get_view_name
        assert duck_pond.get_view_name(dataset, "data") == '"EP Physiology_Data"'
        assert duck_pond.get_view_name(dataset, "events") == '"EP Physiology_Events"'

        # Test invalid table type
        try:
            duck_pond.get_view_name(dataset, "invalid")
            assert False, "Should have raised ValueError"
        except ValueError as e:
            assert "Invalid table_type" in str(e)

        # Test list_dataset_views
        views = duck_pond.list_dataset_views(dataset)
        expected_views = [
            '"EP Physiology_Data"',
            '"EP Physiology_Events"',
        ]
        assert views == expected_views

        # Test list_all_views
        all_views = duck_pond.list_all_views()
        assert (
            len(all_views) >= 4
        )  # At least test_dataset + EP Physiology views (2 views each)
        assert '"EP Physiology_Data"' in all_views
        assert '"test_dataset_Data"' in all_views


class TestDuckPondS3Configuration:
    """Test S3 backend configuration"""

    def test_local_filesystem_default(self, temp_warehouse):
        """Test that local filesystem is used by default"""
        duck_pond = DuckPond(warehouse_path=temp_warehouse)

        assert not duck_pond.use_s3
        assert duck_pond.warehouse_path == temp_warehouse

    def test_s3_configuration_detection(self):
        """Test that S3 configuration is properly detected"""
        duck_pond = DuckPond(
            s3_endpoint="https://s3.example.com",
            s3_access_key="test_key",
            s3_secret_key="test_secret",
            s3_bucket="test-bucket",
        )

        assert duck_pond.use_s3
        assert duck_pond.s3_endpoint == "https://s3.example.com"
        assert duck_pond.s3_access_key == "test_key"
        assert duck_pond.s3_secret_key == "test_secret"
        assert duck_pond.s3_bucket == "test-bucket"
        assert duck_pond.s3_region == "us-east-1"  # Default
        assert duck_pond.warehouse_path == "s3://test-bucket/iceberg-warehouse"

    def test_partial_s3_config_falls_back_to_local(self, temp_warehouse):
        """Test that partial S3 config falls back to local filesystem"""
        duck_pond = DuckPond(
            warehouse_path=temp_warehouse,
            s3_endpoint="https://s3.example.com",
            # Missing other S3 parameters
        )

        assert not duck_pond.use_s3
        assert duck_pond.warehouse_path == temp_warehouse

    def test_from_environment_with_s3_vars(self, monkeypatch):
        """Test from_environment method with S3 environment variables"""
        monkeypatch.setenv("S3_ENDPOINT", "https://ceph.example.com")
        monkeypatch.setenv("S3_ACCESS_KEY", "env_key")
        monkeypatch.setenv("S3_SECRET_KEY", "env_secret")
        monkeypatch.setenv("S3_BUCKET", "env-bucket")
        monkeypatch.setenv("S3_REGION", "eu-west-1")

        duck_pond = DuckPond.from_environment()

        assert duck_pond.use_s3
        assert duck_pond.s3_endpoint == "https://ceph.example.com"
        assert duck_pond.s3_access_key == "env_key"
        assert duck_pond.s3_secret_key == "env_secret"
        assert duck_pond.s3_bucket == "env-bucket"
        assert duck_pond.s3_region == "eu-west-1"

    def test_from_environment_with_local_path(self, monkeypatch):
        """Test from_environment method with local warehouse path"""
        monkeypatch.setenv("LOCAL_ICEBERG_PATH", "/test/warehouse")

        duck_pond = DuckPond.from_environment()

        assert not duck_pond.use_s3
        assert duck_pond.warehouse_path == "/test/warehouse"

    def test_from_environment_defaults(self):
        """Test from_environment method with no environment variables"""
        import os

        # Clear any existing env vars that might interfere
        s3_vars = [
            "S3_ENDPOINT",
            "S3_ACCESS_KEY",
            "S3_SECRET_KEY",
            "S3_BUCKET",
            "LOCAL_ICEBERG_PATH",
            "CONTAINER_ICEBERG_PATH",
        ]
        original_values = {}
        for var in s3_vars:
            original_values[var] = os.environ.pop(var, None)

        try:
            duck_pond = DuckPond.from_environment()

            assert not duck_pond.use_s3
            assert duck_pond.warehouse_path == "./local_iceberg_warehouse"  # Default

        finally:
            # Restore original environment
            for var, value in original_values.items():
                if value is not None:
                    os.environ[var] = value


class TestDuckPondEvents:
    """Test get_events() method functionality"""

    @pytest.fixture
    def events_schema(self):
        """Create the PyArrow schema for events matching DuckPond's Iceberg schema"""
        return pa.schema(
            [
                pa.field("dataset", pa.string(), nullable=False),
                pa.field("animal", pa.string(), nullable=False),
                pa.field("deployment", pa.string(), nullable=False),
                pa.field("recording", pa.string(), nullable=True),
                pa.field("group", pa.string(), nullable=True),
                pa.field("event_key", pa.string(), nullable=False),
                pa.field("datetime_start", pa.timestamp("us"), nullable=False),
                pa.field("datetime_end", pa.timestamp("us"), nullable=False),
                pa.field("short_description", pa.string(), nullable=True),
                pa.field("long_description", pa.string(), nullable=True),
                pa.field("event_data", pa.string(), nullable=False),
            ]
        )

    @pytest.fixture
    def sample_events(self, events_schema):
        """Create sample event data"""
        return pa.table(
            [
                pa.array(["test_dataset"] * 3),  # dataset
                pa.array(["seal_001", "seal_001", "seal_002"]),  # animal
                pa.array(["deploy_001", "deploy_001", "deploy_002"]),  # deployment
                pa.array(["rec_001", "rec_001", "rec_002"]),  # recording
                pa.array(["dive", "dive", "surface"]),  # group
                pa.array(["deep_dive", "shallow_dive", "surface_rest"]),  # event_key
                pa.array(
                    [
                        pd.Timestamp("2024-01-01T10:00:00"),
                        pd.Timestamp("2024-01-01T11:00:00"),
                        pd.Timestamp("2024-01-01T12:00:00"),
                    ]
                ),  # datetime_start
                pa.array(
                    [
                        pd.Timestamp("2024-01-01T10:05:00"),
                        pd.Timestamp("2024-01-01T11:03:00"),
                        pd.Timestamp("2024-01-01T12:10:00"),
                    ]
                ),  # datetime_end
                pa.array(
                    ["Dive to 100m", "Shallow dive to 20m", "Resting at surface"]
                ),  # short_description
                pa.array(
                    [
                        "Long deep dive description",
                        "Long shallow dive description",
                        "Long surface rest description",
                    ]
                ),  # long_description
                pa.array(
                    ['{"depth": 100}', '{"depth": 20}', '{"depth": 0}']
                ),  # event_data
            ],
            schema=events_schema,
        )

    def test_get_events_basic(self, duck_pond, sample_events):
        """Test basic event retrieval"""
        # Write events to the database
        duck_pond.write_to_iceberg(sample_events, "events", dataset="test_dataset")

        # Get all events
        events_df = duck_pond.get_events(dataset="test_dataset")

        # Verify we got the correct number of events
        assert len(events_df) == 3

        # Verify columns are present
        expected_columns = [
            "dataset",
            "animal",
            "deployment",
            "recording",
            "group",
            "event_key",
            "datetime_start",
            "datetime_end",
            "short_description",
            "long_description",
            "event_data",
        ]
        for col in expected_columns:
            assert col in events_df.columns

        # Verify data is correct
        assert events_df["event_key"].tolist() == [
            "deep_dive",
            "shallow_dive",
            "surface_rest",
        ]

    def test_get_events_with_animal_filter(self, duck_pond, sample_events):
        """Test event filtering by animal ID"""
        duck_pond.write_to_iceberg(sample_events, "events", dataset="test_dataset")

        # Get events for seal_001 only
        events_df = duck_pond.get_events(dataset="test_dataset", animal_ids="seal_001")

        assert len(events_df) == 2
        assert all(events_df["animal"] == "seal_001")

    def test_get_events_with_multiple_filters(self, duck_pond, sample_events):
        """Test event filtering by multiple criteria"""
        duck_pond.write_to_iceberg(sample_events, "events", dataset="test_dataset")

        # Get events for seal_001 and deployment_001
        events_df = duck_pond.get_events(
            dataset="test_dataset", animal_ids="seal_001", deployment_ids="deploy_001"
        )

        assert len(events_df) == 2
        assert all(events_df["animal"] == "seal_001")
        assert all(events_df["deployment"] == "deploy_001")

    def test_get_events_with_event_key_filter(self, duck_pond, sample_events):
        """Test event filtering by event key"""
        duck_pond.write_to_iceberg(sample_events, "events", dataset="test_dataset")

        # Get only deep_dive events
        events_df = duck_pond.get_events(dataset="test_dataset", event_keys="deep_dive")

        assert len(events_df) == 1
        assert events_df["event_key"].iloc[0] == "deep_dive"

    def test_get_events_date_range(self, duck_pond, sample_events):
        """Test event filtering by date range"""
        duck_pond.write_to_iceberg(sample_events, "events", dataset="test_dataset")

        # Get events that overlap with a specific time range
        events_df = duck_pond.get_events(
            dataset="test_dataset",
            date_range=("2024-01-01T10:00:00", "2024-01-01T11:30:00"),
        )

        # Should get the first two events (deep_dive and shallow_dive)
        assert len(events_df) == 2
        assert events_df["event_key"].tolist() == ["deep_dive", "shallow_dive"]

    def test_get_events_date_range_overlap(self, duck_pond, sample_events):
        """Test that date range correctly catches overlapping events"""
        duck_pond.write_to_iceberg(sample_events, "events", dataset="test_dataset")

        # Query a range that partially overlaps with the first event
        events_df = duck_pond.get_events(
            dataset="test_dataset",
            date_range=("2024-01-01T10:02:00", "2024-01-01T10:03:00"),
        )

        # Should still get the deep_dive event because it overlaps
        assert len(events_df) == 1
        assert events_df["event_key"].iloc[0] == "deep_dive"

    def test_get_events_with_limit(self, duck_pond, sample_events):
        """Test limiting the number of results"""
        duck_pond.write_to_iceberg(sample_events, "events", dataset="test_dataset")

        # Get only first 2 events
        events_df = duck_pond.get_events(dataset="test_dataset", limit=2)

        assert len(events_df) == 2

    def test_get_events_empty_table(self, duck_pond):
        """Test get_events with no data returns empty DataFrame"""
        # Get events from empty table
        events_df = duck_pond.get_events(dataset="test_dataset")

        # Should return empty DataFrame
        assert len(events_df) == 0
        assert isinstance(events_df, pd.DataFrame)

    def test_get_events_no_matches(self, duck_pond, sample_events):
        """Test get_events returns empty DataFrame when no events match filter"""
        duck_pond.write_to_iceberg(sample_events, "events", dataset="test_dataset")

        # Query for non-existent animal
        events_df = duck_pond.get_events(dataset="test_dataset", animal_ids="seal_999")

        assert len(events_df) == 0
        assert isinstance(events_df, pd.DataFrame)

    def test_get_events_list_filters(self, duck_pond, sample_events):
        """Test that list filters work correctly"""
        duck_pond.write_to_iceberg(sample_events, "events", dataset="test_dataset")

        # Query with list of animal IDs
        events_df = duck_pond.get_events(
            dataset="test_dataset", animal_ids=["seal_001", "seal_002"]
        )

        assert len(events_df) == 3
        assert set(events_df["animal"].unique()) == {"seal_001", "seal_002"}

    def test_get_events_ordered_by_start_time(self, duck_pond, sample_events):
        """Test that events are returned ordered by start time"""
        duck_pond.write_to_iceberg(sample_events, "events", dataset="test_dataset")

        events_df = duck_pond.get_events(dataset="test_dataset")

        # Verify chronological order
        start_times = pd.to_datetime(events_df["datetime_start"])
        assert all(
            start_times[i] <= start_times[i + 1] for i in range(len(start_times) - 1)
        )
