"""
Test Data Uploader

Tests generated by Claude 4 Sonnet.
"""

import pytest
import tempfile
import xarray as xr
import numpy as np
import pandas as pd
from DiveDB.services.data_uploader import DataUploader
from DiveDB.services.data_uploader import NetCDFValidationError
from DiveDB.services.duck_pond import DuckPond


@pytest.fixture
def valid_netcdf_dataset():
    time_samples = pd.date_range("2023-01-01", periods=5, freq="D")
    variable_labels = ["label1", "label2"]

    ds = xr.Dataset(
        {
            "data_var1": (("signal_samples",), np.random.rand(5)),
            "data_var2": (
                ("signal_samples", "signal_variables"),
                np.random.rand(5, 2),
            ),
        },
        coords={
            "signal_samples": ("signal_samples", time_samples),
            "signal_variables": ("signal_variables", variable_labels),
        },
    )

    # Set the necessary attributes
    ds["data_var1"].attrs["variable"] = "example_variable"
    ds["data_var2"].attrs["variables"] = ["label1", "label2"]

    return ds


@pytest.fixture
def temp_warehouse():
    """Create a temporary directory for Iceberg warehouse"""
    with tempfile.TemporaryDirectory() as temp_dir:
        yield temp_dir


@pytest.fixture
def duck_pond(temp_warehouse):
    """Create a DuckPond instance for testing"""
    return DuckPond(warehouse_path=temp_warehouse)


class TestDataUploaderConstructor:
    """DataUploader Constructor Changes"""

    def test_constructor_with_default_duck_pond(self, monkeypatch, temp_warehouse):
        """Test DataUploader constructor creates DuckPond with environment variable"""
        # Set the environment variable
        monkeypatch.setenv("LOCAL_ICEBERG_PATH", temp_warehouse)

        # Create DataUploader - should create DuckPond automatically
        uploader = DataUploader()

        # Verify DuckPond was created
        assert uploader.duck_pond is not None
        assert isinstance(uploader.duck_pond, DuckPond)
        assert uploader.duck_pond.warehouse_path == temp_warehouse

    def test_constructor_with_default_fallback(self):
        """Test DataUploader constructor falls back to default path when env var not set"""
        # Create DataUploader without setting environment variable
        uploader = DataUploader()

        # Verify DuckPond was created with default path
        assert uploader.duck_pond is not None
        assert isinstance(uploader.duck_pond, DuckPond)
        assert uploader.duck_pond.warehouse_path == "./local_iceberg_warehouse"

    def test_constructor_with_provided_duck_pond(self, duck_pond):
        """Test DataUploader constructor with explicitly provided DuckPond"""
        # Create DataUploader with provided DuckPond
        uploader = DataUploader(duck_pond=duck_pond)

        # Verify the provided DuckPond is used
        assert uploader.duck_pond is duck_pond
        assert isinstance(uploader.duck_pond, DuckPond)


def test_validate_data_valid(valid_netcdf_dataset, duck_pond):
    """Test NetCDF validation with valid data"""
    uploader = DataUploader(duck_pond=duck_pond)
    ds = valid_netcdf_dataset
    assert uploader.validate_netcdf(ds)


def test_validate_data_invalid(valid_netcdf_dataset, duck_pond):
    """Test NetCDF validation with invalid data"""
    uploader = DataUploader(duck_pond=duck_pond)
    ds = valid_netcdf_dataset
    ds = ds.rename({"signal_samples": "parent_signal"})

    with pytest.raises(NetCDFValidationError):
        uploader.validate_netcdf(ds)


class TestDataUploaderEvents:
    """Test event data upload functionality"""

    @pytest.fixture
    def event_netcdf_dataset(self):
        """Create a netCDF dataset with event data"""
        # Create time coordinates for events
        event_times = pd.date_range("2023-01-01T10:00:00", periods=4, freq="H")

        # Create event data with both point and state events
        event_data_key = ["dive_start", "surface", "feeding_bout", "rest_period"]
        event_data_value = event_times  # Start times
        event_data_duration = [
            0,
            0,
            1800,
            3600,
        ]  # Point events have 0 duration, state events have > 0

        ds = xr.Dataset(
            {
                "event_data_key": (("event_data_samples",), event_data_key),
                "event_data_value": (("event_data_samples",), event_data_value),
                "event_data_duration": (("event_data_samples",), event_data_duration),
            },
            coords={
                "event_data_samples": ("event_data_samples", event_times),
            },
        )

        return ds

    def test_write_events_to_duck_pond_consolidation(self, duck_pond):
        """Test that both point and state events are written to single events table"""
        import pyarrow as pa
        from datetime import datetime, timedelta

        uploader = DataUploader(duck_pond=duck_pond)
        dataset = "test_events_dataset"
        duck_pond.ensure_dataset_initialized(dataset)

        # Create test data with mixed point and state events
        start_time = datetime(2023, 1, 1, 10, 0, 0)
        start_times = pa.array(
            [
                start_time,  # dive_start (point)
                start_time + timedelta(minutes=30),  # surface (point)
                start_time + timedelta(hours=1),  # feeding_bout (state)
                start_time + timedelta(hours=3),  # rest_period (state)
            ]
        )

        end_times = pa.array(
            [
                start_time,  # dive_start: same time = point event
                start_time + timedelta(minutes=30),  # surface: same time = point event
                start_time
                + timedelta(hours=1, minutes=30),  # feeding_bout: +30 min = state event
                start_time + timedelta(hours=4),  # rest_period: +1 hour = state event
            ]
        )

        # Set up test metadata
        metadata = {
            "animal": "seal_001",
            "deployment": "deploy_001",
            "recording": "rec_001",
        }

        # Write mixed events to consolidated table
        uploader._write_events_to_duck_pond(
            dataset=dataset,
            metadata=metadata,
            start_times=start_times,
            end_times=end_times,
            group="behavioral",
            event_keys=["dive_start", "surface", "feeding_bout", "rest_period"],
            event_data=[
                {"depth": 10},
                {"depth": 0},
                {"prey_type": "fish"},
                {"location": "surface"},
            ],
            short_descriptions=[None, None, "Feeding", "Resting"],
            long_descriptions=[
                None,
                None,
                "Active feeding bout",
                "Rest period at surface",
            ],
        )

        # Refresh views to make data visible in queries
        duck_pond.dataset_manager._create_dataset_views(dataset)

        # Verify all events were written to single events table
        view_name = duck_pond.get_view_name(dataset, "events")
        results = duck_pond.conn.sql(
            f"SELECT event_key, datetime_start, datetime_end FROM {view_name} ORDER BY datetime_start"
        ).fetchall()

        # Should have 4 events total
        assert len(results) == 4

        # Verify point events (dive_start, surface) have same start/end times
        assert (
            results[0][1] == results[0][2]
        ), "dive_start should be point event (same start/end)"
        assert (
            results[1][1] == results[1][2]
        ), "surface should be point event (same start/end)"

        # Verify state events (feeding_bout, rest_period) have different start/end times
        assert (
            results[2][1] != results[2][2]
        ), "feeding_bout should be state event (different start/end)"
        assert (
            results[3][1] != results[3][2]
        ), "rest_period should be state event (different start/end)"

    def test_events_schema_compatibility(self, duck_pond):
        """Test that the events schema handles both point and state events"""
        import pyarrow as pa
        from datetime import datetime, timedelta

        dataset = "test_schema_dataset"
        duck_pond.ensure_dataset_initialized(dataset)

        # Create test data with both point and state events
        start_time = datetime(2023, 1, 1, 12, 0, 0)

        # Point event (same start/end time)
        point_start = pa.array([start_time])
        point_end = pa.array([start_time])  # Same time = point event

        # State event (different start/end time)
        state_start = pa.array([start_time + timedelta(hours=1)])
        state_end = pa.array(
            [start_time + timedelta(hours=2)]
        )  # Different time = state event

        uploader = DataUploader(duck_pond=duck_pond)

        # Test writing point event with nullable descriptions
        uploader._write_events_to_duck_pond(
            dataset=dataset,
            metadata={"animal": "seal_test", "deployment": "deploy_test"},
            start_times=point_start,
            end_times=point_end,
            group="behavioral",
            event_keys=["dive_start"],
            event_data=[{"depth": 10.5}],
            short_descriptions=[None],  # Point events can have null descriptions
            long_descriptions=[None],
        )

        # Test writing state event with descriptions
        uploader._write_events_to_duck_pond(
            dataset=dataset,
            metadata={"animal": "seal_test", "deployment": "deploy_test"},
            start_times=state_start,
            end_times=state_end,
            group="behavioral",
            event_keys=["feeding_bout"],
            event_data=[{"prey_type": "fish"}],
            short_descriptions=["Feeding behavior"],
            long_descriptions=["Extended feeding bout with multiple prey captures"],
        )

        # Refresh views to make data visible in queries
        duck_pond.dataset_manager._create_dataset_views(dataset)

        # Verify both events are in the same table
        view_name = duck_pond.get_view_name(dataset, "events")
        results = duck_pond.conn.sql(
            f"SELECT event_key, datetime_start, datetime_end, short_description FROM {view_name}"
        ).fetchall()

        assert len(results) == 2

        # Check point event
        point_result = [r for r in results if r[0] == "dive_start"][0]
        assert point_result[1] == point_result[2]  # start == end for point event
        assert point_result[3] is None  # null description allowed

        # Check state event
        state_result = [r for r in results if r[0] == "feeding_bout"][0]
        assert state_result[1] != state_result[2]  # start != end for state event
        assert state_result[3] == "Feeding behavior"  # description present
